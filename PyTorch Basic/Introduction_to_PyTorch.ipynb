{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "AnrIRq0bSJWr",
        "lpF3-1IUSOwD",
        "n4yccrBQm5ND",
        "red_i25xs6pX",
        "YMmJkxjauuk1",
        "2sAusUqX0Afg",
        "-zGPiV_N2VOs"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# QuickStart"
      ],
      "metadata": {
        "id": "AnrIRq0bSJWr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Working with data\n",
        "- ```torch.utils.data.DataSet``` stores the samples and their corresponding labels\n",
        "- ```torch.utils.data.DataLoader``` wraps an iterable around the ```Dataset```\n",
        "- ```torchvision.datasets``` module contains ```Dataset``` objects and includes tow arguments: ```transform``` and ```target_transform``` to modify the samples and labels respectively"
      ],
      "metadata": {
        "id": "tOmb4yY_J1mN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4vyfwpCuJpsI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download training data from open datasets\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root = \"data\",\n",
        "    train = True,\n",
        "    download = True,\n",
        "    transform = ToTensor(),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root = \"data\",\n",
        "    train = False,\n",
        "    download = True,\n",
        "    transform = ToTensor(),\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0Dq5fSCLBLk",
        "outputId": "5bae9839-d8eb-404a-d6e9-582c1a4b2c53"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:02<00:00, 12310274.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 210564.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:01<00:00, 3895218.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 13102109.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We pass the ```Dataset``` as an argument to ```DataLoader```. This wraps an iterable over our datasets, and support automatic batching, sampling, shuffling, and multiprocess data loading."
      ],
      "metadata": {
        "id": "wC__q6SKJ1kG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64 # each element in the dataloader iterable will return a batch of 64 features and labels\n",
        "\n",
        "# Create data loaders\n",
        "train_dataloader = DataLoader(training_data, batch_size = batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size = batch_size)\n",
        "\n",
        "for X, y in test_dataloader:\n",
        "  print(f\"Shape of ≈ X [N, C, H, W]: {X.shape}\")\n",
        "  print(f\"Shape of ≈ y: {y.shape} {y.dtype}\")\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "looNoWS3L7Xz",
        "outputId": "f8ad875b-31e2-47d2-9a89-3e4ac84971a5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of ≈ X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of ≈ y: torch.Size([64]) torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Models\n",
        "To define a nn in PyTorch, we create a class that inherits from ```nn.Module```.  \n",
        "- ```___init___()``` : define the layers of the nn \n",
        "- ```forward()``` : specify how data will pass through the network"
      ],
      "metadata": {
        "id": "iGhZwXWLMtgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get cpu, gpu or mps device for training\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# Define model\n",
        "class NeuralNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.linear_relu_stack = nn.Sequential(\n",
        "        nn.Linear(28*28, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 10)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.flatten(x)\n",
        "    logits = self.linear_relu_stack(x)\n",
        "    return logits\n",
        "\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xcm8VCanMmR2",
        "outputId": "77c20cab-a207-49fd-b1cc-d308af32dd0b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizing the Model Parameters"
      ],
      "metadata": {
        "id": "NJiBoathOP8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-3)"
      ],
      "metadata": {
        "id": "grltP1m6OJOo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- train \n",
        "  - makes predictions on the training dataset\n",
        "  - backpropagates the prediction error to adjust the model's parameters"
      ],
      "metadata": {
        "id": "Fz1Lt7KWOeBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  model.train()\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    # Compute prediction error\n",
        "    pred = model(X)\n",
        "    loss = loss_fn(pred, y)\n",
        "\n",
        "    # Backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      loss, current = loss.item(), (batch + 1) * len(X)\n",
        "      print(f\"loss : {loss:>7f} [{current:>5d}/{size:>5d}]\")"
      ],
      "metadata": {
        "id": "xKNQo9AeObFS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(dataloader, model, loos_fn):\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "  model.eval()\n",
        "  test_loss, correct = 0,0 \n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      pred = model(X)\n",
        "      test_loss += loss_fn(pred, y).item()\n",
        "      correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "  test_loss /= num_batches\n",
        "  correct /= size\n",
        "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, avg loss: {test_loss:>8f}\\n\")"
      ],
      "metadata": {
        "id": "FJ7tE489Oz23"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t +1}\\n----------------------------------------\")\n",
        "  train(train_dataloader, model, loss_fn, optimizer)\n",
        "  test(test_dataloader, model, loss_fn)\n",
        "  \n",
        "print(\"Done\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vB3IlwGfQPdB",
        "outputId": "0e954fde-1a78-45b2-a6a0-c3da19477945"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "----------------------------------------\n",
            "loss : 2.316387 [   64/60000]\n",
            "loss : 2.300258 [ 6464/60000]\n",
            "loss : 2.278380 [12864/60000]\n",
            "loss : 2.272400 [19264/60000]\n",
            "loss : 2.240164 [25664/60000]\n",
            "loss : 2.220697 [32064/60000]\n",
            "loss : 2.231028 [38464/60000]\n",
            "loss : 2.193226 [44864/60000]\n",
            "loss : 2.198538 [51264/60000]\n",
            "loss : 2.155947 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 43.3%, avg loss: 2.154220\n",
            "\n",
            "Epoch 2\n",
            "----------------------------------------\n",
            "loss : 2.169596 [   64/60000]\n",
            "loss : 2.161458 [ 6464/60000]\n",
            "loss : 2.097149 [12864/60000]\n",
            "loss : 2.117272 [19264/60000]\n",
            "loss : 2.056101 [25664/60000]\n",
            "loss : 2.000705 [32064/60000]\n",
            "loss : 2.034314 [38464/60000]\n",
            "loss : 1.948967 [44864/60000]\n",
            "loss : 1.959059 [51264/60000]\n",
            "loss : 1.881269 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 57.3%, avg loss: 1.886926\n",
            "\n",
            "Epoch 3\n",
            "----------------------------------------\n",
            "loss : 1.920177 [   64/60000]\n",
            "loss : 1.900504 [ 6464/60000]\n",
            "loss : 1.774097 [12864/60000]\n",
            "loss : 1.818250 [19264/60000]\n",
            "loss : 1.698489 [25664/60000]\n",
            "loss : 1.654297 [32064/60000]\n",
            "loss : 1.680757 [38464/60000]\n",
            "loss : 1.573518 [44864/60000]\n",
            "loss : 1.599641 [51264/60000]\n",
            "loss : 1.489539 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 60.3%, avg loss: 1.517977\n",
            "\n",
            "Epoch 4\n",
            "----------------------------------------\n",
            "loss : 1.581464 [   64/60000]\n",
            "loss : 1.561303 [ 6464/60000]\n",
            "loss : 1.401785 [12864/60000]\n",
            "loss : 1.476657 [19264/60000]\n",
            "loss : 1.345490 [25664/60000]\n",
            "loss : 1.349848 [32064/60000]\n",
            "loss : 1.366715 [38464/60000]\n",
            "loss : 1.282153 [44864/60000]\n",
            "loss : 1.319564 [51264/60000]\n",
            "loss : 1.216220 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 63.1%, avg loss: 1.250814\n",
            "\n",
            "Epoch 5\n",
            "----------------------------------------\n",
            "loss : 1.325541 [   64/60000]\n",
            "loss : 1.319486 [ 6464/60000]\n",
            "loss : 1.144597 [12864/60000]\n",
            "loss : 1.253970 [19264/60000]\n",
            "loss : 1.118474 [25664/60000]\n",
            "loss : 1.150993 [32064/60000]\n",
            "loss : 1.174616 [38464/60000]\n",
            "loss : 1.101857 [44864/60000]\n",
            "loss : 1.146697 [51264/60000]\n",
            "loss : 1.055812 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 64.6%, avg loss: 1.084959\n",
            "\n",
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving Models\n",
        "- serialize the internal state dictionary(containing the model parameter) : a common way to save a model"
      ],
      "metadata": {
        "id": "Sla9hZ8HQqsx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"model.pth\")\n",
        "print('Saved PyTorch Model State to model.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yb3k7IEHQkLB",
        "outputId": "36fe7403-f2ba-4905-82b4-f99dabba54f2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved PyTorch Model State to model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Model\n",
        "The process for loading a model includes re-creating the model structure and loading the state dictionary into it"
      ],
      "metadata": {
        "id": "-H1Cq26hRD9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNetwork().to(device)\n",
        "model.load_state_dict(torch.load(\"model.pth\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhMC5PpYRC5D",
        "outputId": "7c3244e2-db70-468c-d881-9f100e7bcdf0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# used to make prediction\n",
        "classes = [\n",
        "    \"T-shirt/top\",\n",
        "    \"Trouser\",\n",
        "    \"Pullover\",\n",
        "    \"Dress\",\n",
        "    \"Coat\",\n",
        "    \"Sandal\",\n",
        "    \"Shirt\",\n",
        "    \"Sneaker\",\n",
        "    \"Bag\",\n",
        "    \"Ankle boot\"\n",
        "]\n",
        "\n",
        "model.eval()\n",
        "x, y = test_data[0][0], test_data[0][1]\n",
        "with torch.no_grad():\n",
        "  x = x.to(device)\n",
        "  pred = model(x)\n",
        "  predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "  print(f\"Predicted: '{predicted}', Actual: '{actual}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXr6qPUDRV4i",
        "outputId": "292a0060-782c-46bb-a975-090c9cf3a4c3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: 'Ankle boot', Actual: 'Ankle boot'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensors"
      ],
      "metadata": {
        "id": "lpF3-1IUSOwD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initializing a Tensor"
      ],
      "metadata": {
        "id": "_E8QjhDuiblZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "siv-RCLMSQlR"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [[1, 2], [3, 4]]\n",
        "x_data = torch.tensor(data)"
      ],
      "metadata": {
        "id": "U0lNkXHliLnV"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np_array = np.array(data)\n",
        "x_np = torch.from_numpy(np_array)"
      ],
      "metadata": {
        "id": "0UnRy2GpiRgv"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
        "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
        "\n",
        "x_rand = torch.rand_like(x_data, dtype = torch.float) # overrides the datatype of x_data\n",
        "print(f\"Random Tensor: \\n {x_rand} \\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIDb2gJOiYYO",
        "outputId": "b6d6a89b-a5ed-472c-cfe2-5456a772096b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ones Tensor: \n",
            " tensor([[1, 1],\n",
            "        [1, 1]]) \n",
            "\n",
            "Random Tensor: \n",
            " tensor([[0.7121, 0.3797],\n",
            "        [0.7286, 0.6256]]) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shape = (2, 3, )\n",
        "rand_tensor = torch.rand(shape)\n",
        "ones_tensor = torch.ones(shape)\n",
        "zeros_tensor = torch.zeros(shape)\n",
        "\n",
        "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
        "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
        "print(f\"Zeros Tensor: \\n {zeros_tensor} \\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "538z2JM9i32y",
        "outputId": "71f7c003-02d9-4cfe-af5f-dffe847a6959"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Tensor: \n",
            " tensor([[0.3274, 0.7052, 0.3046],\n",
            "        [0.9286, 0.8899, 0.7434]]) \n",
            "\n",
            "Ones Tensor: \n",
            " tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]]) \n",
            "\n",
            "Zeros Tensor: \n",
            " tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]]) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attributes of a Tensor"
      ],
      "metadata": {
        "id": "-N61f00UjU-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor = torch.rand(3, 4)\n",
        "\n",
        "print(f\"Shape of tensor: {tensor.shape}\")\n",
        "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
        "print(f\"Device tensor is stored on: {tensor.device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nU80q3MojSBS",
        "outputId": "102eaf12-15f9-43ae-da5d-44bc2ab64f05"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of tensor: torch.Size([3, 4])\n",
            "Datatype of tensor: torch.float32\n",
            "Device tensor is stored on: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Operations on Tensors"
      ],
      "metadata": {
        "id": "1ZKifthXjwBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We move our tensor to the GPU uf available\n",
        "if torch.cuda.is_available():\n",
        "  tensor = tensor.to(\"cuda\")"
      ],
      "metadata": {
        "id": "S0flWFzZjr3P"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- standard numpy-like indexing and slicing"
      ],
      "metadata": {
        "id": "trU6k3u4kfaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor = torch.ones(4, 4)\n",
        "print(f\"First row: {tensor[0]}\")\n",
        "print(f\"First column: {tensor[:, 0]}\")\n",
        "print(f\"Last column: {tensor[..., -1]}\")\n",
        "tensor[:, 1] = 0\n",
        "print(tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-cc_yPHj9rf",
        "outputId": "1e662818-15a5-4fe0-fe6a-4adc62cc921c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First row: tensor([1., 1., 1., 1.])\n",
            "First column: tensor([1., 1., 1., 1.])\n",
            "Last column: tensor([1., 1., 1., 1.])\n",
            "tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- joining tensors"
      ],
      "metadata": {
        "id": "iHDCgVrnkj96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t1 = torch.cat([tensor, tensor, tensor], dim = 1)\n",
        "print(t1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpU_e30ckWlQ",
        "outputId": "25902592-a9da-4181-b609-a94a6e899769"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Arithmatic operations"
      ],
      "metadata": {
        "id": "rpFZAboWkz-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# matrix multiplication\n",
        "y1 = tensor @ tensor.T\n",
        "y2 = tensor.matmul(tensor.T)\n",
        "\n",
        "y3 = torch.rand_like(y1)\n",
        "torch.matmul(tensor, tensor.T, out = y3)\n",
        "\n",
        "\n",
        "# element-wise product\n",
        "z1 = tensor * tensor\n",
        "z2 = tensor.mul(tensor)\n",
        "\n",
        "z3 = torch.rand_like(tensor)\n",
        "torch.mul(tensor, tensor, out = z3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2Xjl6mIkrN9",
        "outputId": "0ca54e3e-7477-4378-948c-e77c76522d93"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 1., 1.],\n",
              "        [1., 0., 1., 1.],\n",
              "        [1., 0., 1., 1.],\n",
              "        [1., 0., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- single-element tensors\n",
        "\n",
        "```\n",
        "tensor([[1., 0., 1., 1.],\n",
        "        [1., 0., 1., 1.],\n",
        "        [1., 0., 1., 1.],\n",
        "        [1., 0., 1., 1.]])\n",
        "```"
      ],
      "metadata": {
        "id": "t1roetrdljCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agg = tensor.sum()\n",
        "agg_item = agg.item()\n",
        "print(agg_item, type(agg_item))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8tTTi4vlbNy",
        "outputId": "d92f8c3a-33ad-4db5-d99a-7156632a1ad8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12.0 <class 'float'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- in-place operations"
      ],
      "metadata": {
        "id": "ghijGts5l7hf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"{tensor} \\n\")\n",
        "tensor.add_(5)\n",
        "print(tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_U94x5kltrS",
        "outputId": "d9febd54-e273-4a13-ce42-357986920fbc"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]]) \n",
            "\n",
            "tensor([[6., 5., 6., 6.],\n",
            "        [6., 5., 6., 6.],\n",
            "        [6., 5., 6., 6.],\n",
            "        [6., 5., 6., 6.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bridge with Numpy"
      ],
      "metadata": {
        "id": "mcFUB2u6mK_j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Tensor to Numpy array"
      ],
      "metadata": {
        "id": "45Dy1Y_YmOOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t = torch.ones(5)\n",
        "print(f\"t: {t}\")\n",
        "n = t.numpy()\n",
        "print(f\"n: {n}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGGbn5HomJMj",
        "outputId": "c020a5ef-0427-47cc-b690-43ab539541e1"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t: tensor([1., 1., 1., 1., 1.])\n",
            "n: [1. 1. 1. 1. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t.add_(1)\n",
        "print(f\"t: {t}\")\n",
        "print(f\"n: {n}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBrk3-6dmboZ",
        "outputId": "8429f98f-3900-4798-8be2-b2e22614d706"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t: tensor([2., 2., 2., 2., 2.])\n",
            "n: [2. 2. 2. 2. 2.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Numpy array to tensor"
      ],
      "metadata": {
        "id": "g0pSznLAmmGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = np.ones(5)\n",
        "t = torch.from_numpy(n)"
      ],
      "metadata": {
        "id": "EMfkLvw4mkH6"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.add(n, 1, out=n)\n",
        "print(f\"t: {t}\")\n",
        "print(f\"n: {n}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFnSas5zmvEy",
        "outputId": "4e0b3eb7-279b-4544-bc9a-cb0180a26db8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
            "n: [2. 2. 2. 2. 2.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataSets & DataLoaders"
      ],
      "metadata": {
        "id": "n4yccrBQm5ND"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading a Dataset\n",
        "- ```root``` : path where the train/test data is stored\n",
        "- ```train``` : specifies training or test dataset\n",
        "- ```download=True``` : downloads the data from the internet if it's not available at ```root```\n",
        "- ```transform``` and ```target_transform``` : specifiy the feature and label transformations"
      ],
      "metadata": {
        "id": "50FyItyxnBg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root = \"data\",\n",
        "    train = True,\n",
        "    download = True,\n",
        "    transform = ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root = \"data\",\n",
        "    train = False,\n",
        "    download = True,\n",
        "    transform = ToTensor()\n",
        ")"
      ],
      "metadata": {
        "id": "MQ9p5Mg8m3Md"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Iterating and Visualizing the Dataset"
      ],
      "metadata": {
        "id": "SGXPyelUoJvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels_map = {\n",
        "    0: \"T-Shirt\",\n",
        "    1: \"Trouser\",\n",
        "    2: \"Pullover\",\n",
        "    3: \"Dress\",\n",
        "    4: \"Coat\",\n",
        "    5: \"Sandal\",\n",
        "    6: \"Shirt\",\n",
        "    7: \"Sneaker\",\n",
        "    8: \"Bag\",\n",
        "    9: \"Ankle Boot\",\n",
        "}\n",
        "\n",
        "figure = plt.figure(figsize = (8, 8))\n",
        "cols, rows = 3, 3\n",
        "\n",
        "for i in range(1, cols * rows + 1):\n",
        "  sample_idx = torch.randint(len(training_data), size = (1, )).item()\n",
        "  img, label = training_data[sample_idx]\n",
        "  figure.add_subplot(rows, cols, i)\n",
        "  plt.title(labels_map[label])\n",
        "  plt.axis(\"off\")\n",
        "  plt.imshow(img.squeeze(), cmap = \"gray\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "id": "Mx9N0x6uoG-w",
        "outputId": "20050558-95c4-4b98-b59a-1da27d263bb7"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 9 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAn4AAAKSCAYAAABMVtaZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABiyElEQVR4nO3deXRV9dX/8R0CGcgAIYQZEiDIWJUKOIs4oYAoFapiFSe0VVFqrY+1z3KmVh9rRRR9bBUcKg5VfNSKiIqKA2pVQJyYEZCZDIQAYbi/P7rMr5Tv5wv3cDN+36+1XEv2yb7n3HPPsLlk75MUi8ViBgAAgHqvQU1vAAAAAKoHhR8AAEAgKPwAAAACQeEHAAAQCAo/AACAQFD4AQAABILCDwAAIBAUfgAAAIGg8AMAAAgEhR+AIE2ePNmSkpLsn//85z5/9vjjj7fjjz++6jcKAKoYhV8V+fGm8u//tWjRwgYMGGDTpk2r6c0Daq3/PG/Uf++8844zf/fu3fbEE0/Y4Ycfbs2aNbOsrCw76KCD7IILLrDZs2dX+fZ//fXXdsstt9iyZcuqfF1AdeK+Vj80rOkNqO9uu+0269ixo8ViMVu7dq1NnjzZBg0aZK+88ooNGTKkpjcPqHWefPLJPf78xBNP2IwZM/aKd+/e3Zl/9dVX24MPPmhnnHGGnXfeedawYUP77rvvbNq0adapUyc74ogj4t6mN954Y79/9uuvv7Zbb73Vjj/+eCsoKIh7XUBtx32tbqPwq2KnnXaa9enTp/LPl1xyibVs2dKmTJnCCQI4/OIXv9jjz7Nnz7YZM2bsFXdZu3atTZw40UaPHm2PPPLIHsvuu+8+W79+faRtSklJ2efPbNu2bb9+DqjruK/VbfxTbzVr2rSppaenW8OG/7/mvueee+yoo46y3NxcS09Pt8MOO8z+/ve/75W7detWu/rqq6158+aWlZVlQ4cOtVWrVllSUpLdcsst1fgugNpp6dKlFovF7Oijj95r2Y//LPWftm/fbtdee63l5eVZRkaGDRs2bK8C8T9/x++dd96xpKQke+aZZ+y///u/rW3btta4cWO7//77bcSIEWZmNmDAgH3+szRQH3Bfq1v4xq+KlZSU2IYNGywWi9m6detswoQJVlZWtse3F+PHj7ehQ4faeeedZxUVFfbMM8/YiBEj7NVXX7XBgwdX/tyFF15ozz33nJ1//vl2xBFH2LvvvrvHciB0+fn5Zmb2/PPP24gRI6xx48b7zBkzZozl5OTYzTffbMuWLbP77rvPrrrqKnv22Wf3mXv77bdbSkqKXXfddbZ9+3Y75ZRT7Oqrr7b777/fbrzxxsp/jlb/LA3URdzX6rgYqsSkSZNiZrbXf6mpqbHJkyfv8bPl5eV7/LmioiLWq1ev2AknnFAZ++yzz2JmFhs7duweP3vhhRfGzCx28803V9l7AWrSlVdeGYvnUnXBBRfEzCyWk5MTGzZsWOyee+6JffPNN3v93I/n6EknnRTbvXt3ZfzXv/51LDk5OVZcXFwZ69+/f6x///6Vf545c2bMzGKdOnXa6/x9/vnnY2YWmzlz5v6/SaAO4L5WP/BPvVXswQcftBkzZtiMGTPsqaeesgEDBtill15qL774YuXPpKenV/5/UVGRlZSU2LHHHmuff/55Zfz11183M7Mrrrhij9cfM2ZMFb8DoG6ZNGmSPfDAA9axY0ebOnWqXXfddda9e3c78cQTbdWqVXv9/GWXXWZJSUmVfz722GNt165dtnz58n2ua9SoUXucv0AIuK/VbfxTbxXr16/fHr8Ee+6551rv3r3tqquusiFDhlhKSoq9+uqrdscdd9icOXNs+/btlT/77zej5cuXW4MGDaxjx457vH5hYWHVvwmglikrK7OysrLKPycnJ1teXp6ZmTVo0MCuvPJKu/LKK23jxo32wQcf2MMPP2zTpk2zc845x2bNmrXHa3Xo0GGPP+fk5JjZv25W+/Kf5yMQAu5rdRvf+FWzBg0a2IABA2z16tW2cOFCmzVrlg0dOtTS0tJs4sSJ9tprr9mMGTNs5MiRFovFanpzgVrpnnvusdatW1f+17dvX+fP5ebm2tChQ+21116z/v372/vvv7/XN3nJycnO3P05//i2D+C+VtfwjV8N2Llzp5n961uLF154wdLS0mz69OmWmppa+TOTJk3aIyc/P992795tS5cutS5dulTGFy1aVD0bDdQiF1xwgR1zzDGVf96fAqxPnz727rvv2urVqyubQKrCv3+jAYSC+1rdwTd+1WzHjh32xhtvWEpKinXv3t2Sk5MtKSnJdu3aVfkzy5Yts5deemmPvIEDB5qZ2cSJE/eIT5gwocq3GahtOnXqZCeddFLlfz+Ob1mzZo19/fXXe/18RUWFvfXWW9agQYMq/2ekjIwMMzMrLi6u0vUAtQX3tbqFb/yq2LRp0+zbb781M7N169bZ008/bQsXLrQbbrjBsrOzbfDgwXbvvffaqaeeaiNHjrR169bZgw8+aIWFhTZv3rzK1znssMPsrLPOsvvuu882btxY2fa+YMECM+NbBsDMbOXKldavXz874YQT7MQTT7RWrVrZunXrbMqUKTZ37lwbO3asNW/evEq34dBDD7Xk5GS76667rKSkxFJTU+2EE05wzhAE6iLua3UbhV8Vu+mmmyr/Py0tzbp162YPPfSQXX755WZmdsIJJ9ijjz5qf/zjH23s2LHWsWNHu+uuu2zZsmV7nCBm/3p0VatWrWzKlCk2depUO+mkk+zZZ5+1rl27WlpaWrW+L6A26tq1q91333322muv2cSJE23t2rWWlpZmvXr1sr/85S92ySWXVPk2tGrVyh5++GG788477ZJLLrFdu3bZzJkzKfxQb3Bfq9uSYvymZZ02Z84c6927tz311FN23nnn1fTmAABwQLivVS1+x68O2bp1616x++67zxo0aGDHHXdcDWwRAADRcV+rfvxTbx1y991322effWYDBgywhg0b2rRp02zatGl22WWXWfv27Wt68wAAiAv3terHP/XWITNmzLBbb73Vvv76aysrK7MOHTrY+eefb7///e/3eDg2AAB1Afe16kfhBwAAEAh+xw8AACAQFH4AAACBoPADAAAIxH7/5iQTtFEf1cZfcQ3lXDvttNPksrvvvtsZLy0tlTkbN250xn2fcUVFhTPeuHFjmdO7d29n/IUXXpA5//M//+OMf//99zKnvuFcq1saNNDfC+3evTvu1xs7dqwzfsghh8gcdb5fc801ca8/OTlZLvv3R8vVB/s61/jGDwAAIBAUfgAAAIGg8AMAAAgEhR8AAEAg9nuAM78Ei/qIXziPj/oF6euvv17m3HTTTc54WlqazCkqKnLGfU0XKSkpzvjOnTtljnoygO+4WLdunTOemZkpc9SyzZs3y5w///nPzvitt94qc6L80n114VyrndQ+iPJ5jRs3Ti57++23nfG33npL5vz85z93xu+8806Z07lzZ7lMUY0stfl88qG5AwAAAGZG4QcAABAMCj8AAIBAUPgBAAAEgsIPAAAgEBR+AAAAgWCcC4LGiIm9jR8/Xi5T4xWaNGkic0pKSpzxbdu2xbdhZrZ161a5LDU11RkvLy+Pez2+sTHquZ6NGjWSOeo4842AUeNuduzYIXPUM0yfeuopmVNdONfiW79alugRI2oMknqOtZnZ1Vdf7YwXFBTInGuvvTau7fKZMGGCXLZ27Vpn/I477pA5UfZBbcY4FwAAAJgZhR8AAEAwKPwAAAACQeEHAAAQCAo/AACAQNDVi6DVl07DKA8ZP++885zxxx57TOaojjlfp2lycnJccTOznTt3OuO+faO6hH3rUa+3fft2mdOwYUNnXHXhmunPwXf8qRxf97By6KGHymVFRUVxv14U9eVcq2nq+DPT70d1optF6xJ+6aWXnPHp06fLnIceesgZV934Zvo87Nmzp8yZMmWKM37wwQfLHMV3fKjPwXe9UeeAut6Z+T+7eNfzI77xAwAACASFHwAAQCAo/AAAAAJB4QcAABAICj8AAIBAUPgBAAAEQveFJ0Bubq4zvnHjRpmj2rRvuukmmbNkyRJn/Ntvv5U569atc8bVw5rNdGu3r+VbtXb7Hv6sWrGjtIn7WuWbNm0qlyn9+/d3xjMyMmTOmWeeGfd6EiknJ6dG118dooxk+PWvf+2MFxcXx72eRI8yyc7OdsZ9Y2OysrKccd84BJXjez9qu9U1xUyPmvFtmzrf1WuZ6XP6wQcflDkjR46Uy7A3db1P9LiaJk2aOOMlJSUJXU8UeXl5zrjvHFB8o5OUr776Si5r3Lhx3K+n+D5TdS3yXaNqC77xAwAACASFHwAAQCAo/AAAAAJB4QcAABAICj8AAIBAHHBXb2FhoVz29NNPO+OqI8jM7OOPP3bGfd22J598sjOemZkpc1q0aOGMt2nTRuaohzL71qO6an1dsGo9vg4j1QHo6/bcvHmzM75161aZozoKVQeamdkbb7zhjG/atEnmqP3j6zhVx9Utt9wic+q7c889Vy7r2rWrM+7bx+o89HXmqWOzZcuWMueHH35wxsvKymTOl19+Gfe2qXNtzZo1MmfBggXO+C9/+UuZ07ZtW2d8y5YtMkeda76OY/V6Rx99tMxRncC+4wB7a9BAf4+irsMFBQUyR123Jk6cKHPKy8udcd95s2LFCmfc13GuzhvfNb26qM/h0EMPlTlz5sxxxlu1aiVzmjVr5oz36NFD5nTs2NEZ/9vf/iZz1LXwQPCNHwAAQCAo/AAAAAJB4QcAABAICj8AAIBAUPgBAAAEgsIPAAAgEEmx/XyytHow9cCBA2VOfn6+M37NNdfInFWrVjnjaiSEmVlFRUXcOWpUgm+UiXq9Ro0ayRy1TG2zL8c3mkXl+MasqJEpvtE56gHUvrEU2dnZzrhvH+zcuTPubVMjC4466iiZk+gHqyeCOtei8I0lUcezOjfM9GdZVFQkc6KMNFLrUa9lpo9N33nToUMHZ1yNxfCtZ/HixTJHHZu+a5Q6p3zntMrJysqSOWrc0vnnny9zoqgv55rK8b0/dZypkUpmZuPGjXPG1fgdMz2CRV1PzfRYJd85vXHjRme8pKRE5sydO9cZ913T1bVIjUcyM/vJT37ijPvuUWqsjhpbY6bHrLRu3VrmqM/hkksukTkzZsyQy5R9nWt84wcAABAICj8AAIBAUPgBAAAEgsIPAAAgEBR+AAAAgdAtK//h4YcfdsZnzZolc1SHj+/hz6qLx/fwZ9Vl5XsIfPPmzZ1xXzeMWo+vM0x1BW3evDnu9fgeAq46o3wdx6rLyfdAb/XZ+fabegi4LycvL88Z93VZqU6zEKhOMt8+Vh3svuNZvZ5vPepB56qb1MfX0agewj59+nSZc8455zjjarqAmdmoUaOc8dzcXJlTXFzsjPuua6qz2dcFuX379rjWb2Z2+OGHO+O+rmtfh2R9F6U7efny5c54ixYtZM6GDRuccV/XvaKup2ZmixYtcsZV97qZ3gft2rWTOaoT19cJrK736enpMkd15Pvun99//70z7jvX0tLSnHH1uZnpa251d7zzjR8AAEAgKPwAAAACQeEHAAAQCAo/AACAQFD4AQAABILCDwAAIBD7Pc5FjWTo0qWLzFm7dq0z7mstV+3OvtEsiWyF9j00XY258I0YUa/ne2i64muvVw+ijzKexvdQe/V+fDmNGzd2xlNTU2WOej3fw8bViJ4Q3HHHHc6477xRY3Z845YqKiqccd94BXUMXnvttTJH8Y0aWrhwoTP+u9/9TuaocVTvv/++zFHjIv7617/KHDW24+yzz5Y5apyLGtlips8P33Wtffv2zvif/vQnmfPLX/5SLgvVYYcdJpetXr3aGV+/fr3M6d27tzO+YMECmaOOTd91QF2ffSN71HnoOzbVaLEmTZrInCj3G3X98uX4RlgpvvEwihoPE6UeOBB84wcAABAICj8AAIBAUPgBAAAEgsIPAAAgEBR+AAAAgdjvrt7//u//dsZHjhwpc1TXpq+7RnUF+R4YrjplfOtRnYa+Th3VLeTrnFXb4Ot+Uq/n68xTncW+TmDVyeR7MHWUTmDVaejr0FWv5+seVV1jIbjqqquccV+32tFHH+2Mqw5+M31O+zrbzz33XGf8iiuukDlqme94Vp2TL7/8sszxde8q6mHzo0aNkjndunVzxn0PdFcd1JmZmTJHdQeuWbNG5owfP94Zv/7662UO9jZ69Gi5zDfBQFH3CN+5VlRUFPd60tLSnHE1YcNMdwJH6bb1XbdVjjo3zPR0B9+5pjqYfR3Har9t27ZN5qjrly+nKvCNHwAAQCAo/AAAAAJB4QcAABAICj8AAIBAUPgBAAAEgsIPAAAgEPs9zmX+/PnO+Lp162SOejB0aWmp3iDRqu4bSxFlNItqO0/0g5yjbFuU9ajt9o2AUcuiPHw6yufj29eqxV+19/tyQqBGL1x88cVxv9ZJJ50kl/3sZz9zxn/1q1/JnLfeessZ/+yzz2SOGlniG5nhG/GgPPbYY874RRddFPdrbdq0SS5btWqVM+7b5vbt2zvjvpE2t912mzOuRrYgcdR4DzOzZs2axf16ubm5zrjvnqvGghQXF8scNZrFNzZI3ad9I7rU/vHdb9Qy3z5Qo4t842nUe40yasZHjejx3QurAt/4AQAABILCDwAAIBAUfgAAAIGg8AMAAAgEhR8AAEAg9rurV/E9FPqggw5yxn2dP4qv80d1p0bplPF1p6ptiNKhqzpdfaK8nygduonuMFLb4Os4TklJccZ9nVS+LjTsvzfffDPuZeecc47M6d+/vzP+wgsvyJxRo0Y546oD0czsvffec8YPP/xwmaMsXrxYLovSzafO94yMDJkzd+5cZ/z444+Pe/2oer169ZLLlixZEvfrqeuZ7/hT92PV7Wum78epqakyR127fcez6mD3dUOrLthWrVrJHNX17rvfqC5l374uLy93xn01hHq9KPfpA8E3fgAAAIGg8AMAAAgEhR8AAEAgKPwAAAACQeEHAAAQCAo/AACAQBzwOJeysjK5rG3bts74li1bZE6UtmbVPq1atM30yBLfKBPVDu5r31bLooyEqO6W76rm229q/IUvJysryxn3jQvA3nzHmTo/VqxYIXOefvppZ/yss86SOXfeeacz7huZobZt0aJFMkeNmEhPT5c5Uc7DzZs3x70etW1RRDnX4DZ27FhnvGvXrjLnxRdfTNj6fePQ1D3Kd5xt3brVGffdP9UoE9/YGLVtvrExatvUmBfftuXk5MicTZs2OeO+ETBqmW+/KWp8WVWpX5UEAAAAJAo/AACAQFD4AQAABILCDwAAIBAUfgAAAIE44K5eX4duo0aNnHH1EGUzf/dRvHwduqqrNtHdb77XC4XaB1G6R32fqeoOS+QxFQLfPlbmz58vl/Xs2dMZV92+ZrqjdfHixTJHXVd8Xd3qGhWl695HHeu+Dk3fsnjRuZs4ubm5ceesX78+YeuPct30nQOqo7S0tFTmZGZmOuMZGRkyR/Gda+r89HUPq67e/Px8maP2m+oqNjNr2rRpXOs30/tHTaSoKnzjBwAAEAgKPwAAgEBQ+AEAAASCwg8AACAQFH4AAACBoPADAAAIxAGPc/E9LFm1O/vGuagHH/vGoqh2cN9YCjXeIEqrfBSJfK2oojxsXvGNi1APrfY9ADvKKAv1fhL5PkPge8i4+lx857QazbJmzRqZo85338PM1bIo1wEf9XpRjjPf+qv7we3YP59//rkzrkaPmJmtWrUqYev3jTJR11TfcaaWNWnSROb47vuKGnPiu96oe7tvX6vRKL79ps5p32iW5s2bO+O+64B6P759UBW4IwIAAASCwg8AACAQFH4AAACBoPADAAAIBIUfAABAIA64lcTXzZeXl+eMr1ixQubk5OQ4474uIvUgZd/Dn6urY05tQ5RuwiidWb5u6ESKsh5ft2UiOyerax/UF1H2l69jTn1mvs681NRUZ9zX7a2W+Y6ZRB4bUc5pn+p+cDv2T3FxsTPuO85UThS+e6G6r/nu0+r1fOdGbm6uM+7bB2rKh2+6Q5TpG+q9+u436lrUrFkzmaOueb5tUzm+WqUq8I0fAABAICj8AAAAAkHhBwAAEAgKPwAAgEBQ+AEAAASCwg8AACAQBzzOpaioSC5LS0uLK26mH+i+ZcsWmRNllImvvb06RBkx4Xs/6vV86/G1tydSIsdc+LZZjQXwHW/YW5TRAtnZ2XG/nu94Vst8x1IiR/34Xksti7Jtvn3tGyURL9++TvQYmvpO3Yt89xQ1nigK33rUOLTNmzfLHHVsNG7cWOZEGbMS5b7WsKG7RPGdN2o0i2/b1L3DN/ZN3Yt8I6fUNqj3WVX4xg8AACAQFH4AAACBoPADAAAIBIUfAABAICj8AAAAAnHArSSLFi2Sy6I8/Lm0tNQZ93VzqtfzdcqorqTq6nCL8n6idPVWV+euT3Vtg+rmoqs3Pr7jTPF1LUbpto3SNVgbjnUlkR2NqFnLli1zxn3Hn28qRby2bdsmlzVt2tQZz8rKkjmqS9h3bKrpG75jNkp3fyL5Pp8o1yglyrSC6r5H8Y0fAABAICj8AAAAAkHhBwAAEAgKPwAAgEBQ+AEAAASCwg8AACAQBzwvQI1sMTPbunWrM+57oHtxcbEz3rx5c5mjWsujPCw5ysPMo4yAUQ+F3tc21EVq/0TZ11FG2mRkZHi2Donge2i6GqPgG6EQ5ZxSrxfltaKcg75xEWrUUHWNj0Li+O4rSm5ubsLWn56eLpepsSC+40wdt1GOTV9OIkem+NaT6HM33m3wvU/1+TRp0iTu9R8IvvEDAAAIBIUfAABAICj8AAAAAkHhBwAAEAgKPwAAgEAccFev6lYzM9u0aZMzXlhYKHPKysqc8dLSUpmjHgzt6+5RXbW+ji2V4+vQVd1Cvhxfh6SS6A6sePn2tXo/vu6nRHZo+rrgkBjqQe9m+tioru71KOvx5STynPatp3HjxnJZvKJ00MMtSndqQUFBwtav7ndm+hj0fcYpKSlxvZZvG3w5UaZiROm2VdsQ5bV8onT1qn2dyHN9f/CNHwAAQCAo/AAAAAJB4QcAABAICj8AAIBAUPgBAAAEgsIPAAAgEAc8ziU7O1suUyMEVq5cKXOaNWvmjPvGRaj2bV9btRqv4Bv9odq3t23bFndOokcoJPJB9FHGX/j2tXq9KK3/vlE3CxcujHs9SIwoo5N8oozMUOMafK8VZcRDlPWosVe+8VG+UVmoORs2bHDGffco37J4bd++XS7LzMx0xn0jYNS568uJcn5W1/imRI5tifI+fVR9kZOTk9D17Avf+AEAAASCwg8AACAQFH4AAACBoPADAAAIBIUfAABAIA64q1d1EZnpBw+XlZXJHNX95MtRXTy+Dt20tLS41m+W2Acsb968Oe4cX+dulK5elZPoh9qrzsUonaC+rl71er4c7C2Rx5JZ9XXzJboDL5GiPNA90Z3/qFq+Du05c+ZUyzaoTnDfNVBda9U90kzfc337QHUJJ/rakcj7WqKlpqY64/n5+dW6HbX3SgkAAICEovADAAAIBIUfAABAICj8AAAAAkHhBwAAEAgKPwAAgEAc8DiXUaNGyWXt27d3xhctWiRzWrVq5YxXVFTInPLycmc8yoOxfe3oqh3c99Ds1q1bO+PNmjWTOVHa3tUy3wOrVYu/r+1dLfPlqG3wjQtYtWqVM+4bS6BGCx1yyCEyB3VLIsc7JJpvNIu6fqnxDmZm27ZtO+BtQvUpLi6Wy0488URn/G9/+1vc6/HdO9Tx5DsH1AgYFTfT56HvPh3lPIxyj1LjaaKMjYkyPizKWKnOnTvLZer1fPf2feEbPwAAgEBQ+AEAAASCwg8AACAQFH4AAACBoPADAAAIxAF39fq6khYuXBj362VnZzvjLVq0kDm5ubnOuOruMTNLSUlxxn0dU+r1MjIyZM6MGTOc8bvvvlvmROmyitIxlciHVkfpGmvcuLHMUdvmW4/q6v3www9lzsMPPyyXhSrKceHrMPOdh0qUTrYo54DKSXQHfZR9kJ6eHncOak6PHj3ksnHjxjnjp59+usxRHbLqHmmmJ0JE6dBV90jfenznQJQu1CjnZ5TrV5T7jbpG+aaJqNdT+9PMLC8vzxlfu3atzNkXvvEDAAAIBIUfAABAICj8AAAAAkHhBwAAEAgKPwAAgEBQ+AEAAATigMe5LF68ONIyALVTlHEIUUYYJHob1Hp8rxXlQetRHsIeZSzF8uXL416PksjRTXDbuXOnXHbbbbc54998843MWbdunTPuG4OVmpoqlynq2CgtLZU5aWlpzrhvLElZWVlc6zeLdu2IIsr5EWWkjTpGSkpKZE5VjHXiGz8AAIBAUPgBAAAEgsIPAAAgEBR+AAAAgaDwAwAACMQBd/UCqF+idNJt3bpVLvN1/Cq7du1yxqM8BN73ftR6ouT4ujpVt6V6LbNoD7VHzTnllFPksieffNIZ93Vzqs/fd8yoY7CiokLmqGN9xYoVMqdHjx7OeEpKisxRncBRunp952d1dbAnsuO4devWctnQoUOd8fvvvz/y+vjGDwAAIBAUfgAAAIGg8AMAAAgEhR8AAEAgKPwAAAACQeEHAAAQCMa5ANhDgwb674NqlETLli1ljhrj4HugvFrmG5kSZbyCyklOTk7Ya5mZbdmyxRnPysqSOR07dox7G5Tqeth9yHJzc+UydU6tX79e5mRnZzvj6enpMicjIyOuuG9ZXl6ezNm2bZsz7huloq4dvuuNOm59OQ0busuaKOdAlNEwvtFWS5cudcb79Okjc7766qu4t2Ff+MYPAAAgEBR+AAAAgaDwAwAACASFHwAAQCAo/AAAAAJBVy+APaiHw/s8+uijclmbNm2c8caNG8sc1VWrOvbMdNeerzNPdSeWl5fHneN7QL3ap5s2bZI5B/IQ9v1dPxJn8eLFctmKFSuccV+XellZmTPu605dvXq1M7527VqZozpNp06dKnM+/PBDuQy1H9/4AQAABILCDwAAIBAUfgAAAIGg8AMAAAgEhR8AAEAgKPwAAAACkRTj6d0AAABB4Bs/AACAQFD4AQAABILCDwAAIBAUfgAAAIGg8AMAAAgEhR8AAEAgKPwAAAACQeEHAAAQCAo/AACAQFD4AQAABILCDwAAIBAUfgAAAIGg8AMAAAgEhV8dNXnyZEtKSrJly5bFnXvhhRdaQUFBwrcJAIAouKdVHwq/OHz55Zc2fPhwy8/Pt7S0NGvbtq2dfPLJNmHChJreNCBIixcvtssvv9w6depkaWlplp2dbUcffbSNHz/etm7dWiXrfPrpp+2+++6rktcGqhP3tDA1rOkNqCs+/PBDGzBggHXo0MFGjx5trVq1shUrVtjs2bNt/PjxNmbMmJreRCAo//jHP2zEiBGWmppqF1xwgfXq1csqKirs/ffft9/+9rf21Vdf2SOPPJLw9T799NM2f/58Gzt2bMJfG6gu3NPCReG3n8aNG2dNmjSxTz/91Jo2bbrHsnXr1tXMRgGBWrp0qZ1zzjmWn59vb7/9trVu3bpy2ZVXXmmLFi2yf/zjHzW4hUDtxj0tXPxT735avHix9ezZc68TxMysRYsWlf8/adIkO+GEE6xFixaWmppqPXr0sIceemivnIKCAhsyZIi9//771q9fP0tLS7NOnTrZE088sdfPfvXVV3bCCSdYenq6tWvXzu644w7bvXv3Xj/3f//3fzZ48GBr06aNpaamWufOne3222+3Xbt2HdibB2qZu+++28rKyuzRRx/do+j7UWFhoV1zzTVmZrZz5067/fbbrXPnzpaammoFBQV244032vbt2/fI2Z/z5/jjj7d//OMftnz5cktKSrKkpCR+twh1Eve0cPGN337Kz8+3jz76yObPn2+9evWSP/fQQw9Zz549bejQodawYUN75ZVX7IorrrDdu3fblVdeucfPLlq0yIYPH26XXHKJjRo1yh577DG78MIL7bDDDrOePXuamdmaNWtswIABtnPnTrvhhhssIyPDHnnkEUtPT99r3ZMnT7bMzEy79tprLTMz095++2276aabrLS01P7nf/4nsTsEqEGvvPKKderUyY466qh9/uyll15qjz/+uA0fPtx+85vf2Mcff2x33nmnffPNNzZ16tTKn9uf8+f3v/+9lZSU2MqVK+3Pf/6zmZllZmZWzZsEqhD3tIDFsF/eeOONWHJyciw5OTl25JFHxq6//vrY9OnTYxUVFXv8XHl5+V65AwcOjHXq1GmPWH5+fszMYu+9915lbN26dbHU1NTYb37zm8rY2LFjY2YW+/jjj/f4uSZNmsTMLLZ06VLvui+//PJY48aNY9u2bauMjRo1Kpafn7/f7x2oTUpKSmJmFjvjjDP2+bNz5syJmVns0ksv3SN+3XXXxcws9vbbb1fG9vf8GTx4MOcP6jzuaeHin3r308knn2wfffSRDR061ObOnWt33323DRw40Nq2bWsvv/xy5c/9+99aSkpKbMOGDda/f39bsmSJlZSU7PGaPXr0sGOPPbbyz3l5eda1a1dbsmRJZey1116zI444wvr167fHz5133nl7beO/r3vz5s22YcMGO/bYY628vNy+/fbbA9sBQC1RWlpqZmZZWVn7/NnXXnvNzMyuvfbaPeK/+c1vzMz2+D1Azh+EhHtauCj84tC3b1978cUXraioyD755BP73e9+Z5s3b7bhw4fb119/bWZmH3zwgZ100kmWkZFhTZs2tby8PLvxxhvNzPY6STp06LDXOnJycqyoqKjyz8uXL7cuXbrs9XNdu3bdK/bVV1/ZsGHDrEmTJpadnW15eXn2i1/8wrluoK7Kzs42s3/dCPZl+fLl1qBBAyssLNwj3qpVK2vatKktX768Msb5g9BwTwsTv+MXQUpKivXt29f69u1rBx10kF100UX2/PPP2y9+8Qs78cQTrVu3bnbvvfda+/btLSUlxV577TX785//vNcvryYnJztfPxaLxb1NxcXF1r9/f8vOzrbbbrvNOnfubGlpafb555/bf/3Xfzl/cRaoi7Kzs61NmzY2f/78/c5JSkryLuf8Qci4p4WFwu8A9enTx8zMVq9eba+88opt377dXn755T3+5jNz5szIr5+fn28LFy7cK/7dd9/t8ed33nnHNm7caC+++KIdd9xxlfGlS5dGXjdQWw0ZMsQeeeQR++ijj+zII4+UP5efn2+7d++2hQsXWvfu3Svja9euteLiYsvPzzez+M6ffRWRQF3GPa3+459699PMmTOdf2v58XeIunbtWvm3nX//uZKSEps0aVLk9Q4aNMhmz55tn3zySWVs/fr19re//W2Pn3Otu6KiwiZOnBh53UBtdf3111tGRoZdeumltnbt2r2WL1682MaPH2+DBg0yM9vrSRv33nuvmZkNHjzYzOI7fzIyMvhnJtR53NPCxTd++2nMmDFWXl5uw4YNs27dullFRYV9+OGH9uyzz1pBQYFddNFFtnbtWktJSbHTTz/dLr/8cisrK7O//OUv1qJFC1u9enWk9V5//fX25JNP2qmnnmrXXHNNZet7fn6+zZs3r/LnjjrqKMvJybFRo0bZ1VdfbUlJSfbkk09G+oodqO06d+5sTz/9tJ199tnWvXv3PZ7c8eGHH9rzzz9vF154oV1zzTU2atQoe+SRRyr/6eiTTz6xxx9/3M4880wbMGCAmcV3/hx22GH27LPP2rXXXmt9+/a1zMxMO/3006t7FwAHhHtawGqmmbjumTZtWuziiy+OdevWLZaZmRlLSUmJFRYWxsaMGRNbu3Zt5c+9/PLLsYMPPjiWlpYWKygoiN11112xxx57bK829fz8/NjgwYP3Wk///v1j/fv33yM2b968WP/+/WNpaWmxtm3bxm6//fbYo48+utdrfvDBB7Ejjjgilp6eHmvTpk1le76ZxWbOnFn5c7S+o75YsGBBbPTo0bGCgoJYSkpKLCsrK3b00UfHJkyYUDnuYceOHbFbb7011rFjx1ijRo1i7du3j/3ud7/bYxxELLb/509ZWVls5MiRsaZNm8bMjHMJdRL3tHAlxWKUzwAAACHgd/wAAAACQeEHAAAQCAo/AACAQFD4AQAABILCDwAAIBAUfgAAAIGg8AMAAAjEfj+5g+dToj6qjWMsa/O5pratNuzH5557zhnftm2bzPn3x0b9u9zcXJnTrl07Z/yuu+6SOYsWLZLLQlEbjpH/VJvPNSCqfZ1rfOMHAAAQCAo/AACAQFD4AQAABILCDwAAIBAUfgAAAIFIiu1nqxXdT6iP6DSMT5Su3kR2Ar/33ntyWX5+vjPeoUMHmbN9+3ZnPCUlRebs3LnTGV+5cqXM6du3rzO+ceNGmdOggfvv5bt375Y5tRnnGlA96OoFAACAmVH4AQAABIPCDwAAIBAUfgAAAIGg8AMAAAgEhR8AAEAgGOeCoNX3ERO+14ry3hM5msXnySefdMZ/8pOfyJxdu3Y54wcddJDMyczMjG/DTI+Uadasmcz5+uuvnfGzzz477vWrMS9mtXvUS30/1+CXkZHhjPuO2bS0NGf85JNPljlFRUXOeJs2bWTO448/7oz7RkGpkU9qrJSZ2ZYtW5zx2bNny5wo11zGuQAAAMDMKPwAAACCQeEHAAAQCAo/AACAQFD4AQAABKJhTW8AgLojkZ2ZRx11lFzWq1cvZ3zp0qUyp6yszBlv0aKFzFFdveXl5TJn/fr1znhpaanMUZ3FF110kcyZNGmSM16bO3cBpbCw0Bnv1KmTzFFdtQcffLDMWbRokTPevXt3mfPDDz8447m5uTJn9erVznhxcbHMUdcBX1dvVXTD840fAABAICj8AAAAAkHhBwAAEAgKPwAAgEBQ+AEAAASCwg8AACAQSbH97BXmYdaoj3hwfGL84x//kMsGDRrkjKvxK2Z6lElWVpbMUQ9H7927t8xp2rSpM+4b5zJv3jxnvKSkRObs2LHDGVcPejczO+WUU5zxBx54QOaMGTNGLqtpnGtIlHbt2sllO3fudMbXrFlTVZuzhx49eshlLVu2dMZ9Y6qWLVsW9zbs61zjGz8AAIBAUPgBAAAEgsIPAAAgEBR+AAAAgaDwAwAACARdvQganYbxWbJkiTO+adMmmdOiRQtnPDs7W+b85S9/ccb79+8vczp37uyMFxUVyRz1QPeCggKZox60Pm3aNJmzceNGZ7xnz54yR3Ujb9++XeasXLnSGe/SpYvMqS6ca3DxfQaJPGYSvZ7hw4c741u2bIl7Pb5zeubMmfFtmGc9P+IbPwAAgEBQ+AEAAASCwg8AACAQFH4AAACBoPADAAAIBIUfAABAIBrW9AYAqDoNGui/2+3evdsZv/rqq2VOx44dnfFPP/1U5rzyyivO+NFHHy1zrr32WmfcN/ZAjUz54YcfZE5FRYUzXlxcLHN27drljJ9yyikyRz1oXX0GZmYPPPCAM96mTRuZM3DgQGf8+uuvlzl33323M15dYzYQNt+xlJyc7Iz7rms7d+6Mez3K0KFD415PeXm5zMnJyXHGp0+fHt+GHSC+8QMAAAgEhR8AAEAgKPwAAAACQeEHAAAQCAo/AACAQCTF9rPVhYdZoz6qjd2JiTzXonRmqi5cM7OmTZs6474Hk/fu3dsZf/bZZ2WO6ra98MILZU5KSoozrrrvzMwWL17sjLdt21bmpKamOuO+Dt23337bGVfdvmZmI0aMcMbz8vJkzvr1653xzZs3y5xDDjlELkuk+n6uoe7q1KmTM+47N8rKypzxwsJCmfPcc88542oigZm+3vgmHOzrXOMbPwAAgEBQ+AEAAASCwg8AACAQFH4AAACBoPADAAAIBIUfAABAIBjngqAxYmJvX375pVxWXFzsjGdnZ8ucjz/+2Bk/8sgj494G3yiTvn37OuO+h6arkQxZWVkyRz0gftOmTTJHjZTp1auXzPn000+dcd8Yh61btzrjXbt2lTn5+flyWSJxrtWcKGOdarPk5GS5TI1V8r3PIUOGOOO7du2SOa1atXLGP/jgA5mzYMECuSyRGOcCAAAAM6PwAwAACAaFHwAAQCAo/AAAAAJB4QcAABCIhjW9AXWJ6oxSD1E2M9u2bZszrh4KbWbWrl07Z/y9997zbF3i+DqmfF1OqFsaNWoUV9zMrGnTps54QUGBzMnMzHTGn3nmGZmTkZHhjKenp8sc1b27Y8cOmaO6lH056vzwbZtaz2OPPSZzUlJSnPGzzz5b5syePdsZT0tLkzlqX2/ZskXmoG6pi527PlHuQ74OetWpr653ZmarVq1yxhPduauurcuWLYv8mnzjBwAAEAgKPwAAgEBQ+AEAAASCwg8AACAQFH4AAACBoPADAAAIBONc/oNq6zbTD39WI1t8jj32WLmsTZs2zvjAgQNlzj//+U9nfOrUqfFtmCV+ZIsafxHlAenqM9jXMuztoIMOcsZ9o0zU6KKioiKZ06JFC2f86KOPljkfffSRM+4bldC7d29nvKKiQuaUlpY64w0b6kujGvHw8ccfy5w5c+Y44wMGDJA56hrhO86zsrKccd+5psZc+N4PEsP3ufjuRUp9G7el9k+U8TS+EWpqX8+fP1/mLF682Blv2bKlzBk+fLgznp+fL3PUeLfbbrtN5uwL3/gBAAAEgsIPAAAgEBR+AAAAgaDwAwAACASFHwAAQCDqdVevr2NKdZru3Lkz7vX813/9l1w2dOhQZ3zy5MkyR3UJd+/eXeYcccQRzviFF14oc1599VVnfMqUKTKnrKxMLlPqW6dZfZGbm+uMl5eXy5xmzZo54xs2bJA5OTk5ca9Hdc76Olq3b9/ujPs6ABs3bhz3etTxXFxcLHMyMzOd8fT0dJmjOqV970d9Pr5zUHUN0tWbOFG6U7luRtOhQwdnvKSkROZ8+OGHznjPnj1lzosvvuiMp6WlyZwok0HUcRCl67syN3ImAAAA6hQKPwAAgEBQ+AEAAASCwg8AACAQFH4AAACBoPADAAAIRK0b56JaoaOMWfHlqGVqxIWZ2ZNPPumMr1y5Uub4HkSfSHl5ec64bwTM2LFjnfHzzjtP5qgRHCkpKTKntLTUGfeNv1Bt777P9IorrojrtUKnHgyuxqKY6dECaiyKmR6dVFhYKHO+/fbbuF7LzL/dSqNGjeJ+rYqKCme8TZs2MmfVqlXOePv27WWOOqd958CKFSuccd/ohxYtWshl2FuU0SxqmW/k2E9/+lNnXI3sMTObMWOGXJZIqampzrjvvPG9V8W3T5WNGzc64126dJE5arRZv379ZM7atWud8R07dsicZcuWOeNq3JOZHqGm3uf+4Bs/AACAQFD4AQAABILCDwAAIBAUfgAAAIGg8AMAAAhErevqVR1zvq60KB2/o0ePdsY7d+4sc4YPH+6M+x42r/i6YNU+8Fm/fn1ccTOzESNGOOO+B1M/9thjzrhvm1WntuqoNNNdoqpr0Yzu3XipLlRf56zaxwUFBTJHPYDc1/128sknO+O+z1+dU77jQj243bcP1PHcsKG+nB533HHOeFZWlsxRXXstW7aUOT/88IMzfsghh8gcXzdyqHwdqKrT9IgjjpA5qgs2PT1d5pxwwgnO+BdffCFzEsm3D6J00Efp0FV69eoll91yyy3O+IABA2SOOtfUdAEz3W3ru7er6+SmTZtkTteuXZ3xdu3ayZx94Rs/AACAQFD4AQAABILCDwAAIBAUfgAAAIGg8AMAAAgEhR8AAEAg9nucixqnEuXB1FHW4xvZcvjhhzvjl1xyicx59913nfEbbrjBs3VuvgfUq4fa+0ZZKL6RNopvlIXabl8b/1133RX3NlSXKA9PD5kaB6BGT5iZFRUVOeP5+fkyZ/Pmzc74li1bZI46333njTrW1TlopkewRBkflZGRIXPUiAc1TsbMrEOHDnJZvJo2bVot6wmZGltkZnbdddc542eddZbMad26tTM+bNgwmaPO6ZUrV8ocNbrId+9QOb5zLYqLLrrIGT/99NNlTvv27Z3xefPmyRw1msU31sk3jkxRr+cbh6auRdnZ2XGvv/I1I2cCAACgTqHwAwAACASFHwAAQCAo/AAAAAJB4QcAABCI/e7q9XX4JJLqmPM9SPy2225zxgcOHJiQbdqX8vJyuSyRnaaJ/gwWLFjgjI8aNSqh66kudO/Gp3Pnzs64r0vd14mrbN261Rn3dQAWFxc7476u3igTATIzM51x30Po1XHm6/IrLS11xnNycmTOJ5984oyrh7abRets9r1efefr2lTUvvzmm29kzvLly53xe+65R+Y0a9bMGVcdqGZm48aNc8Z913TVce7rNI1yL1Lnh+p4NjMbNGiQM75x40aZs3jxYmfc1wWr9oGaSGCmpx+oz83MbO3atc64ukaamc2cOdMZ930++8I3fgAAAIGg8AMAAAgEhR8AAEAgKPwAAAACQeEHAAAQCAo/AACAQOz3OJfCwkJnvG/fvjJHtVWrESdmZnPmzHHGr7rqKpnzwQcfyGVKIses+FTXiJEoD81WoyxmzJgR93rUw+7N/A+8V9S4AN/+TE9Pd8Z9rfIha9KkiTPuG+eSlZUV93rUw+vV8WdmtmHDhrhz1AgWNXbBTI+HifIAdt+xqUbK+Pb1119/7Yz7xnl06tTJGfeNp2nRooVcVt/5ro/x8u3jl19+2Rn3jRw7+uijnfFXXnlF5vzud79zxn3j0H744Qe5LF65ubly2Q033OCMn3DCCTJn1apVzrhvDE/Tpk2d8a+++krmXHHFFc6471x76aWX4lq/mb4X+a5RBx10kDOelpYmc/aFb/wAAAACQeEHAAAQCAo/AACAQFD4AQAABILCDwAAIBD73dWrulGOOuoomXPKKafEvUElJSXOuK9bSHXRPPPMMzInIyPDGfc9BF51+vm6VtXr+boT1YPbfR2Aaht83Ynr1q1zxn37QHVd9+rVS+aoDt1PP/1U5qhuO9UdaWb205/+1Bk/+eSTZU7I1DHo64Lu0KFD3OvZtGmTM666is30A8h93W/qWPd1AKr1+M61KJ2g6sHtxcXFMkd16PrW365dO2fc90B338Pr67tu3bo5475uzpUrV8a9HtWdunz5cpmjro++e8ejjz7qjPs6t1VXr+9cO+OMM5zxMWPGyBzV3a/iZtGmb6h7u69WUR2ypaWlMkedn773k5eX54z77tPff/+9M67O9f3BN34AAACBoPADAAAIBIUfAABAICj8AAAAAkHhBwAAEAgKPwAAgEDs9zgX1Y7ua99W4xqijGZp2FBvqmrFVm3dZv4RD4oao+AblaBGo6xZs0bmbNy4Ma54ol100UVyWUFBgTM+ceJEmaNGWai4T3p6ulx28803O+OzZ8+Oez0hSElJccZ943zUmJMvvvhC5qiRJb6xFOp8940yUeMnfKMf1Kgh34gm9XpRxsb4rlHLli1zxufPny9zevfu7Yz7xlKE7IILLnDGW7ZsKXOifJbqev/+++/LnCijZt555x1n3Dc66frrr3fGo4w/2bx5s8xR2+273ihz586Vy9SYE99+U/cI3z3Xt3+UxYsXO+OqvjLTNVFRUVHc6/8R3/gBAAAEgsIPAAAgEBR+AAAAgaDwAwAACASFHwAAQCD2u6s3ipKSkrjiqB0mTZpU05uABFEPOTfTXaiq09Xngw8+kMu6d+/ujPu6erOyspxxX1ev6ozLz8+XOaqjMMp+83UPq45C1bFnpjub//nPf8ocZfv27XKZ73Oo79Qxk5eXJ3N69erljG/btk3mdO7c2RkfMmSIzFH3yYMPPljmqO7U1q1byxx1bPiuA+p49nWaZmdnO+O+jvO1a9c643379pU5bdq0ccbVFAMzPWnk3XffjXs9vokdbdu2dcZ9+1pdv2bOnClz9oVv/AAAAAJB4QcAABAICj8AAIBAUPgBAAAEgsIPAAAgEBR+AAAAgajScS4AalZ6erpcpkYI+EamKPPnz5fLTj75ZGd8586dMkc9HF2NQzAza9y4sTPeoIH++616r75xLur11LgKM/3wet8YBzXSZt68eTInCt8xUt8tWLDAGT/88MNlzpYtW5xxNXrEzGzTpk3O+DHHHCNz1HiYZ555RuYcdNBBzvjWrVtlzueff+6MZ2RkyJyWLVs644ceeqjMWb9+fVyvZabH0Khz3Uyfaz/72c9kjjoH3nzzTZmj9tu3334rc3Jycpzxn/70pzLnqKOOcsaXLFkic/aFb/wAAAACQeEHAAAQCAo/AACAQFD4AQAABILCDwAAIBB09QKBitK9q/g6zFTn6qpVq+LO8XUCZ2ZmOuPqIfQ+ycnJctmOHTuccV/npOrqVJ2OZmYbNmxwxufMmSNzFN9nrR5e79sHiTx2alJ5ebkz7vssGzVq5IynpqbKnLS0NGc8NzdX5rRr184Z93V1q47WvLw8mdOpUydnvHv37jLnrbfecsZ9XeqqE1d1L5uZNWnSxBmfMWOGzPnNb34jl8Vr1qxZcpn6fPr16ydz1q1b54w3bKhLsU8++cQZ//LLL2XOvvCNHwAAQCAo/AAAAAJB4QcAABAICj8AAIBAUPgBAAAEgsIPAAAgEIxzAeox39iNsrIyZzw7Ozvu9fjGK6hxIX369JE5JSUlzrh6mLqZWWlpqTOuRlyY6TEKat+Y6X2qxsmYmR1yyCHOeNeuXWXOzJkz5bJ4+cZsxGIxZ1x9bmb+cSd1SU5OjjPuG7OixhCpY9bM7NBDD3XGP/jgA71xQkVFhVw2f/58Z3z58uUyR42aUeNXzMyysrKc8bffflvm9O7dO671m5l9/vnnzngiR7b4+K43Bx98cNyvt3HjRmfcd64tXrw47px94Rs/AACAQFD4AQAABILCDwAAIBAUfgAAAIGg8AMAAAgEXb1APebrNFXdgb6uwShatGjhjKvOQDPdUdiggf67qupcVZ2bZmZFRUXOuK+zuby83Blv2bKlzElOTpbLFLXfolCdu2b6/fi6LetLV++aNWuccV9Hq1rmy2nSpIkz3r9/f5lTXFzsjJ900kky5/vvv3fGo3Qpb9iwQeaojtb27dvLHNUp7zunzz77bLmsOvg6tdW0AHVNMdPHwc6dO+PbMDPr2LFj3Dk/4hs/AACAQFD4AQAABILCDwAAIBAUfgAAAIGg8AMAAAgEhR8AAEAgGOcC1GO+0Sxq1ItvLEUUl19+uTN+2WWXyZzS0lJnvGnTpjJHjT/xPcxcjTJRo2HM9JiT9evXy5xNmzY5461atZI5L7zwglymqM97165dMmfhwoXOuG8f1BffffedM/7OO+/InGOPPdYZ941O+uKLL5zxjIwMmaNGf6gxImZ6NMvixYtljhqr5BvZM3/+fGd827ZtMkeNO7rppptkjtoG33gkdaw3atRI5uzYscMZnzNnjsxR10l1TTEzy8vLc8Zbt24tc7Zs2eKMH8jYLb7xAwAACASFHwAAQCAo/AAAAAJB4QcAABAICj8AAIBA0NUL1GO+DkD1cPTCwkKZ8/LLL8e9DVOmTIkrjuhmzZrljKtOVDPdoel7QH19oc6Pu+++W+Y8+OCDzvhxxx0nc372s585476ubnV+qq5yM7OcnBxn3NdxvHPnTme8efPmMkd1fPs66O+9915nfObMmTIn3vUnmu/6qa6Tvs9Hdfw2bKhLsSOOOMIZnzZtmszZF77xAwAACASFHwAAQCAo/AAAAAJB4QcAABAICj8AAIBAUPgBAAAEgnEuQKDUuAgfNS7CRz0cXT0YHf+iPh/fKAs1FsI3ZkM9bD4ESUlJzngsFpM5W7ZsccZ94zXUstTUVJnTp08fZ/zggw+WOe3bt3fGc3NzZU6TJk2c8aKiIpmzevVqZ/ypp56SOUuWLJHLqkOUETAvvPCCXHbQQQc546WlpTJHjXqJMjpp+vTpctnFF1/szeUbPwAAgEBQ+AEAAASCwg8AACAQFH4AAACBoPADAAAIRFLM17707z8oup+Aumw/D/9qVV3n2imnnOKMq241M7NZs2Y543Pnzk3INtVlvi7p6nqo/HHHHeeMH3744TJn0aJFzvjUqVMTsk0/CvlcA6rTvs41vvEDAAAIBIUfAABAICj8AAAAAkHhBwAAEAgKPwAAgEBQ+AEAAARiv8e5AAAAoG7jGz8AAIBAUPgBAAAEgsIPAAAgEBR+AAAAgaDwAwAACASFHwAAQCAo/AAAAAJB4QcAABAICj8AAIBAUPgBAAAEgsIPAAAgEBR+AAAAgaDwAwAACASFXzVLSkqyq666ap8/N3nyZEtKSrJly5ZV/UYBAIAgUPgl0JdffmnDhw+3/Px8S0tLs7Zt29rJJ59sEyZMqPJ1/+EPf7CXXnqpytcD1DY//iXp3/9r0aKFDRgwwKZNm1bTmwfUG5xr9UPDmt6A+uLDDz+0AQMGWIcOHWz06NHWqlUrW7Fihc2ePdvGjx9vY8aMiev1zj//fDvnnHMsNTV1v37+D3/4gw0fPtzOPPPMCFsP1H233XabdezY0WKxmK1du9YmT55sgwYNsldeecWGDBlS05sH1Buca3UbhV+CjBs3zpo0aWKffvqpNW3adI9l69ati/v1kpOTLTk52fszsVjMtm3bZunp6XG/PlDfnHbaadanT5/KP19yySXWsmVLmzJlCjcjIIE41+o2/qk3QRYvXmw9e/bcq+gzM2vRosVesZdeesl69eplqamp1rNnT3v99df3WO76Hb+CggIbMmSITZ8+3fr06WPp6en2v//7v5aUlGRbtmyxxx9/vPLr9wsvvDDB7xCoW5o2bWrp6enWsOH///vtPffcY0cddZTl5uZaenq6HXbYYfb3v/99r9ytW7fa1Vdfbc2bN7esrCwbOnSorVq1ypKSkuyWW26pxncB1H6ca3UL3/glSH5+vn300Uc2f/5869Wrl/dn33//fXvxxRftiiuusKysLLv//vvtrLPOsu+//95yc3O9ud99952de+65dvnll9vo0aOta9eu9uSTT9qll15q/fr1s8suu8zMzDp37pyw9wbUBSUlJbZhwwaLxWK2bt06mzBhgpWVldkvfvGLyp8ZP368DR061M477zyrqKiwZ555xkaMGGGvvvqqDR48uPLnLrzwQnvuuefs/PPPtyOOOMLefffdPZYDIeNcq+NiSIg33ngjlpycHEtOTo4deeSRseuvvz42ffr0WEVFxR4/Z2axlJSU2KJFiypjc+fOjZlZbMKECZWxSZMmxcwstnTp0spYfn5+zMxir7/++l7rz8jIiI0aNSrh7wuo7X48V/7zv9TU1NjkyZP3+Nny8vI9/lxRURHr1atX7IQTTqiMffbZZzEzi40dO3aPn73wwgtjZha7+eabq+y9ALUZ51r9wD/1JsjJJ59sH330kQ0dOtTmzp1rd999tw0cONDatm1rL7/88h4/e9JJJ+3xjdzBBx9s2dnZtmTJkn2up2PHjjZw4MCEbz9Q1z344IM2Y8YMmzFjhj311FM2YMAAu/TSS+3FF1+s/Jl//33YoqIiKykpsWOPPdY+//zzyviPv3ZxxRVX7PH68TZoAfUV51rdxj/1JlDfvn3txRdftIqKCps7d65NnTrV/vznP9vw4cNtzpw51qNHDzMz69Chw165OTk5VlRUtM91dOzYMeHbDdQH/fr12+MXzs8991zr3bu3XXXVVTZkyBBLSUmxV1991e644w6bM2eObd++vfJnk5KSKv9/+fLl1qBBg73OtcLCwqp/E0AdwLlWt/GNXxVISUmxvn372h/+8Ad76KGHbMeOHfb8889XLlfdurFYbJ+vTQcvsH8aNGhgAwYMsNWrV9vChQtt1qxZNnToUEtLS7OJEyfaa6+9ZjNmzLCRI0fu17kHwI1zrW7hG78q9uPfilavXl2l6/n3v0UB+JedO3eamVlZWZm98MILlpaWZtOnT99jPuakSZP2yMnPz7fdu3fb0qVLrUuXLpXxRYsWVc9GA3UQ51rdwTd+CTJz5kzn32Ree+01MzPr2rVrla4/IyPDiouLq3QdQF2yY8cOe+ONNywlJcW6d+9uycnJlpSUZLt27ar8mWXLlu31xJsff4d24sSJe8Sr4wk8QF3EuVa38I1fgowZM8bKy8tt2LBh1q1bN6uoqLAPP/zQnn32WSsoKLCLLrqoStd/2GGH2Ztvvmn33nuvtWnTxjp27GiHH354la4TqE2mTZtm3377rZn9a2j6008/bQsXLrQbbrjBsrOzbfDgwXbvvffaqaeeaiNHjrR169bZgw8+aIWFhTZv3rzK1znssMPsrLPOsvvuu882btxYOWJiwYIFZsa36wDnWh1Xs03F9ce0adNiF198caxbt26xzMzMWEpKSqywsDA2ZsyY2Nq1ayt/zsxiV1555V75+fn5e4xjUeNcBg8e7Fz/t99+GzvuuONi6enpMTNjtAuC4RoxkZaWFjv00ENjDz30UGz37t2VP/voo4/GunTpEktNTY1169YtNmnSpNjNN98c+89L4ZYtW2JXXnllrFmzZrHMzMzYmWeeGfvuu+9iZhb74x//WN1vEagVONfqh6RYjN+0BIB9mTNnjvXu3dueeuopO++882p6c4B6i3OtavE7fgDwH7Zu3bpX7L777rMGDRrYcccdVwNbBNRPnGvVj9/xA4D/cPfdd9tnn31mAwYMsIYNG9q0adNs2rRpdtlll1n79u1revOAeoNzrfrxT70A8B9mzJhht956q3399ddWVlZmHTp0sPPPP99+//vf7/EgegAHhnOt+lH4AQAABILf8QMAAAgEhR8AAEAgKPwAAAACsd+/OVnfJmjffvvtzvjpp58uc8aPH++Mv/feezJn2bJlznhycrLeOKGiokIua968uTN+6aWXypzhw4c743/6059kzpQpU+Syuqg2/oprfTvXlAYN9N87d+/e7YwXFhbKnL59+zrjy5cvlzmZmZnOeH5+vsx54YUXnPFNmzbJHPVL6j8+3zQEnGtA9djXucY3fgAAAIGg8AMAAAgEhR8AAEAgKPwAAAACsd8DnGvzL8EOGzbMGR83bpzM6dixozOumjHMzGbPnu2M79q1S+a0bt3aGb///vtljvpILrvsMpmzceNGZ7xFixYyp0+fPs54Xl6ezFm4cKEz/oc//EHm1OaGEH7hPD5q23z7MUrOIYcc4owfeuihMufxxx+Xy+LVqVMnuezcc891xh944AGZU1JS4oz7PuvaeGweiNr4fmrzuQZERXMHAAAAzIzCDwAAIBgUfgAAAIGg8AMAAAgEhR8AAEAgKPwAAAACUWfGuVx++eVy2cMPP+yMFxcXy5zy8nJnXD0j1MxsyZIlzvhXX30lc7Zt2+aMq7EoZvq5nj/5yU9kTkZGhjN+8MEHyxw16mX79u0yJysryxlX22xmduSRRzrj8+fPlzlRRoBEwYiJmqOek2tm9stf/tIZv+eee+Jej+/YVHzP0O3Vq5cz7nvO95133umMM86lZoVyriEsjHMBAACAmVH4AQAABIPCDwAAIBAUfgAAAIGg8AMAAAhEnenqffnll+Wyrl27OuMNGui6NjU11RmP0mW3a9cumVNWVuaM+7qHGzdu7Iz7Og3V+2nUqJHMUfvHt207duxwxn2dwO+9954z7uvUri50GiaGr3NWHbfnnHOOzFFd75999pnMUce6OmZ90tLS5DLVqX/eeefJnE2bNjnj06ZNkzlqn/quA7UZ51rN8d0Hopwf7du3d8Zvv/12mbNixQpnvHPnzjJn48aNznhFRYXMefTRR53xr7/+WubUN3T1AgAAwMwo/AAAAIJB4QcAABAICj8AAIBAUPgBAAAEgsIPAAAgEPE/vbyKFRQUOONdunSROVu2bHHGc3Jy4l5/enq6XKbGtvjGKzRt2jTubVCt2Gpki5lZcnJy3OtR4yJ8rfLFxcVxr79NmzZxbRdqLzX+wncOtG3b1hlv0aKFzHnmmWfiWr+ZPm98Y51UjhrZ4vPSSy/JZWPHjnXGZ86cKXOibIMSZb+h/ogyssV3zDz//PPO+L333itz3nzzTWd869atMiczM9MZP+aYY2TOxIkTnfG///3vMueBBx6Qy+ojvvEDAAAIBIUfAABAICj8AAAAAkHhBwAAEAgKPwAAgEDUuq7ec8891xn3PWS6tLTUGfd1xakuxB9++EHmqK5eX0drlG7bKHbv3h13TlZWljPue0C96lJW3b5mulPb1+3r+xxQc9Tx7OvqHTx4sDP+7rvvJmz9+9qGRFLXIjVdwMzs888/d8ZPP/10maM6J33XQtW9qTr4fTmoP6IcM7/85S9ljjrWn3vuufg2bB9Ux+/UqVNlzrBhw5zxiy++WOYsWrTIGX/99ddljroWRbkXV3dnPd/4AQAABILCDwAAIBAUfgAAAIGg8AMAAAgEhR8AAEAgKPwAAAACUevGuRx22GHOuBo9YmZWVlbmjBcVFcmc9u3bO+O+VmzVwp6eni5zVKu8r307JSVFLot3Pb4RF6rFf+PGjXGvJycnR+bk5eU542eddZbMmTBhglyGquUb/aGOp8LCQpmjjo25c+fKnAYN3H8nra6RLT5Rxp/MmDHDGf/tb38rc9Q1b/PmzTJHfXaMbAmb7/NXY0mOOeYYmfP3v/897m1Q57TvXhhlzIm63qiRSmZm/fr1c8Z941zUeLe6gG/8AAAAAkHhBwAAEAgKPwAAgEBQ+AEAAASCwg8AACAQta6r929/+5szvn37dpmTkZHhjPu6kqZMmeKMv/rqq56tc0tNTZXLVAeer1tJdQn7uohUN7KvG/rQQw91xm+88UaZox5m/dVXX8mcFStWOOMffPCBzEHd4uvQ/r//+79q3JKao7rkzXRX5RdffCFz1MPmn3jiCZmTlJQkl6F+8H3G6r7iuw+ceuqpznjjxo1lzqxZs+SyeEV5P1H43o/qOK6vwnq3AAAAAaPwAwAACASFHwAAQCAo/AAAAAJB4QcAABAICj8AAIBA1LpxLmlpac74n/70J5nTrl07Z/zdd9+VOVOnTnXG1QOrzXRreUpKisxRbeJq/IqZfhC9egC7mR4X4RsB8+WXXzrjZWVlMkeNp1H708zsiCOOcMZbt24tc1D11PGkjj8zsx49ejjjvuPs22+/dcajjD+pzXz7TfE9BP6GG25wxtu2bStzVq1a5Yz7xlX4rkVIjCj3AXUv9I02U3zjlo466ihnPDMzU+bMnz8/7m1Q+8B3bKrriu96U1RU5Iw3b95c5rRs2dIZP+WUU2TOG2+84Yz7xrtVVFTIZfE6kFE3fOMHAAAQCAo/AACAQFD4AQAABILCDwAAIBAUfgAAAIGodV292dnZznh6errM6dq1qzPet29fmTNv3jxn3NcxFeVBzqr7yNc9rETpBM7IyJA5OTk5zvj3338vcwoLC53xLl26yJwtW7Y4461atZI5qHq+h6MrgwYNcsZnzJgR92sl8gHstYHv/UTpoH711Ved8REjRsic++67Ty5DzYnSOb1t27a4c9T12TcRQk3FeO+99+Jev0+Urvco/vjHPzrjkydPljnr1693xn/yk5/IHNXVG6XrurrxjR8AAEAgKPwAAAACQeEHAAAQCAo/AACAQFD4AQAABILCDwAAIBA1Ms5FjWwxM1u5cqUzXlJSInPKy8ud8Z/+9KcyJyUlxRn3tdCrByyrh2mb6XEuvoc1q9Z/30Pt1XgY38gONSLH146uRsD49oHap75RM0gM3wiiHTt2OOMDBgyQOWp00j333BP3NlTXeIfaQL1X35iN+fPnO+OnnXaazOnVq1dcr+XbhpA+n5pyyCGHyGV9+vRxxn3ndEFBgTM+c+ZMmaNe7+uvv5Y5Bx98sDPuG9HVpEkTZ/ybb76ROeoY3Lp1q8xR+9R3/1Sv58u56KKLnPGioiKZ8+2338YVryp84wcAABAICj8AAIBAUPgBAAAEgsIPAAAgEBR+AAAAgaiRrt42bdrIZZmZmc54s2bNZI7qgn3rrbdkjuq2VZ2OZroT2Ed1tPq6hdR6fF12ah/4OqhLS0ud8caNG8uctWvXOuOqm8zM7Pvvv4972zp16uSML1myROZgb74OQHXM/PznP5c5zz//fNzboLpGfZ3toVCfgc8LL7wgl40cOdIZ93X1ouqdeOKJzvif/vQnmbN69Wpn/PPPP5c56t7Rs2dPmaMmMpx++ukyRx1Pl1xyicyZNWuWMz5kyBCZo64daiKFmVn79u2dcd++VhMm8vLyZI7quv7www9lzrBhw5zxVatWyZwnnnhCLouKb/wAAAACQeEHAAAQCAo/AACAQFD4AQAABILCDwAAIBAUfgAAAIGokXEuvjEeavxJ06ZNZY4a9eIbYaDGqfjGX6Smpjrj27dvj3s9ycnJMkdtg2+cTHl5uVymqDb+LVu2yJzc3FxnPCcnR+bMmzfPGV+5cqXMiTI6B3vzjQA67bTTnPEXX3xR5syYMcMZ9503jG3RfONc1LXDN9JIPWxePbjezGzu3LlyWX2njtsoY3aSkpLkMjWeavjw4TJH3fPUZ2xmdsoppzjjf/nLX2SOGrPiGxtz1FFHOeNqdJeZ2fr1651x32iz999/3xkvLCyUOeresWHDBplz9NFHO+OtWrWSOQsWLHDGp06dKnPUcXXjjTfKnKrAN34AAACBoPADAAAIBIUfAABAICj8AAAAAkHhBwAAEIga6ept3LixXKa6rHbt2iVzVIeRr7tmx44dzrh6KLSZ7try5aiuSl8ncJQHU6uOY99+U91URUVFMkd1U73++usyR22Db7/5jhHsP1+n4eGHH+6MP/DAA3GvJxaLxb0Nvs5233bHy7dtUdTmbXvnnXec8RtuuEHmnHvuuQndhlC99NJLctn999/vjJ911lkyR13TfddnNanh4IMPljmjR492xgsKCmTOY4895oyPGDFC5qxevdoZ/9WvfiVzVIdu7969ZY66R82ZM0fmbNy40Rlv27atzFHTCiZMmCBzVGezbypGz549nfGvvvpK5uwL3/gBAAAEgsIPAAAgEBR+AAAAgaDwAwAACASFHwAAQCAo/AAAAAJRI+NcfA/A3rJlizPue9C7emi5ei0zPcrE91B7NX7C94D6bdu2OeO+USZqPb7RLGqZb/SEylGjbszMysvLnfHMzEyZ4/scFMa5JEb37t3lsqeeesoZ9z3MXIkylsR3riGazz77zBn3jaVo2bKlM7527dqEbFNtpq7dvnuUctppp8llamSJb8zOt99+G1fcTF+fL774YpnTpUsXZ1zdu8zMBg4c6Ix/8cUXMked7757h7quqNEwZmYjR450xp9//nmZoz7vFStWyJyhQ4c6475apVu3bs743LlzZc6ZZ57pjH/33XcyZ1/4xg8AACAQFH4AAACBoPADAAAIBIUfAABAICj8AAAAAlEjXb2+7tQoD3RXXUG+9ajOVV83l+rE9XU/RaH2gepENtPvdfv27XHn+LqUf/jhB2dcPUzbTO8f37Yl+uH1obr99tvlssWLFzvjWVlZMkedN75jc8GCBc74tGnTZI56QL2v41x1DfqOZ5XjW4+vU15JS0tzxn3XG7XM1w2tujpnz54tcx555BFn/IwzzpA59UWUzvJTTz017pyioiJn/K9//avMOeSQQ5zxnJwcmZOfn++M++6fr7zyijOenp4ucw499FBnvLCwUOZs3LjRGe/UqZPMUefap59+KnPeeecdZ/yuu+6SOfPmzXPGV61aJXPUfU2t38ysffv2zniPHj1kjjp2DmQqAt/4AQAABILCDwAAIBAUfgAAAIGg8AMAAAgEhR8AAEAgKPwAAAACUSPjXHzUA45zc3NljholsmnTJpmjxkX4qNZy3+gHxdder7bNN/pBjUbJyMiQOWq/ZWdny5z58+c7482bN5c5zZo1c8Z9+8C3DPtP7XszPZLh+++/lzllZWXOuO/B5C1btnTG+/XrJ3PUCBbfKBV1zBQXF8e9Ht+D49V61LgnX06UcS6+0TlqPV27dpU5s2bNksuwt6eeesoZnzJliswZNGiQM37TTTfJnAkTJjjjeXl5Mmf69OnO+LvvvitzlHHjxsllI0aMcMZPPPFEmdOmTRtnXI1SMdPn4ejRo2WOun/53o/6fN544w2Zc8wxxzjjvXr1kjkrV650xtXIKzOzt99+Wy6Lim/8AAAAAkHhBwAAEAgKPwAAgEBQ+AEAAASCwg8AACAQNdLVqx5ubKa70nwPpt6wYYMzvmvXLpmjHnDsy1Hb5uuyU+/V182nOhd9XYNRHgK/ZcsWZ9z3ftT+UV3FZrp72Hcc+D4H7L8VK1bIZa1bt3bGfftefZY+qnNWHbO+bVCvZabPG1/HsXq9xo0byxx1Hvr2WywWc8Z9+1N19ye64/39999P6OvVB2+99Vbcy3zdl/fee68z/sgjj8gcdX6oLnkzs6FDhzrj33zzjczZuHGjM/773/9e5rRr184Z93XBtmjRwhn3TZEoLS11xj/44AOZo87p2bNnyxzVbXvBBRfIHHVd8V1vpk6d6oyfcMIJMsd3n4yKb/wAAAACQeEHAAAQCAo/AACAQFD4AQAABILCDwAAIBAUfgAAAIGokXEuGRkZclmU0Q9r1qyJOycKNZLB9+B41drtewi82m5fm7gaT6O22UyPhSgvL5c5rVq1csbLyspkjvq8fZ9Poj+7UK1atUouU2NJmjRpktBtiDKax3fcKuo89D3UPsr6fSOSFHVd841mUWOVfGOd1HiYZs2ayRzf6Jr6rlevXs54jx49ZI4ag/TrX/9a5qgxJ2oUmZlZfn6+M/7DDz/IHHXMPProozJHvde+ffvKnJ49ezrjvut2nz59nPFly5bJHHVfu+6662TO//7v/8plSufOnZ1x37YVFhY6475z+vjjj3fG27ZtK3PS09Od8Xnz5smcfeEbPwAAgEBQ+AEAAASCwg8AACAQFH4AAACBoPADAAAIRI109fq6U1UHi+9Bzqpr1NddE+XBx6pjydcFq7rsfF2DUToaFV/Hseq23LFjR9zr2bp1a9w5vu5EdRwgPqorzkx3evqOZ3VORel09YnSQa86Z31dxer1fNcOxZfj2+54X8933qj95suJ8l7ri0MOOcQZf/311+N+rVNPPVUumzNnjjPesmVLmaM6Pa+//nqZo86BTz75ROaoa22LFi1kzvTp051x1e1rZjZlyhRnXHU8m5mtWLHCGb/iiitkzkUXXeSM+zqov/zyS2d81qxZMqd9+/bO+Pnnny9z1Ge6fv16mXPMMcc445MmTZI5+8I3fgAAAIGg8AMAAAgEhR8AAEAgKPwAAAACQeEHAAAQCAo/AACAQNTIOJfZs2fHnTNz5ky57Oc//7kz7nv4eFFRkTOuHnLt4xuZkUi+bVMjK3yjLKKMcWjSpIkz/uabb8oc34OuUbXUeAczPYIlyjihRI8ESeQ4F98oEyXKeqK8nm+/RdmnUcbqRBk1U19kZWU542vWrJE5BQUFzvixxx4rc9TIEt/4k8zMTGdcjQQx0/eI9957T+Y8+uijzvjo0aNljtruW2+9VebcfPPNzrjan2Z6pEzv3r1lTllZmTPuG50zb948Z/yggw6SOaWlpc64b7TZwoULnfGcnByZ06VLF2e8X79+Mmdf+MYPAAAgEBR+AAAAgaDwAwAACASFHwAAQCAo/AAAAAJRI129iZaXl+eM//DDDzJHdbumpKTEvX5fJ12UDkC1bampqTJHdQD6OjQrKiqccV8XpHqvhYWFMoeu3rrF1+WpPn/fca6OTd96onT1qmVRupR9onTOqv3j27Yo+zqKRHdk1yUPP/ywMz5u3DiZc8sttzjj77zzjswpLi52xn3H0sqVK53xX/3qVzJn9erVzvjw4cNlzoYNG5xxX7ftAw884Iz7Ok3V+znjjDNkzrPPPuuMv/322zKndevWzvhbb70lc+6//35nXH1uZnpqiHqfZmZdu3Z1xkeOHClzPv30U2fc95nuC9/4AQAABILCDwAAIBAUfgAAAIGg8AMAAAgEhR8AAEAgKPwAAAACUSPjXKKMcfCNMlHUWBQzPbbFlxNl7EGUURaqxd83ZkWtJ8qYjR07dsgc9QBsNVIHNSuR44TM9DHjG0uhjvVEjyWJMgJGbYMvJ971R329KNQ27Ny5U+b4riv1nboPDBw4UOZ89913zvj3338vcwYNGuSM++4p7dq1c8ZLSkpkTnZ2tjPuOz8XLVrkjF9zzTUyp3379s74cccdJ3OGDBnijP/617+WOVOmTHHGe/fuLXM++OADZ/zkk0+WOWvXrnXGfdfCRo0aOeO+4+CQQw5xxl999VWZ89e//tUZHz9+vMzZF77xAwAACASFHwAAQCAo/AAAAAJB4QcAABAICj8AAIBA1Jl2LtUNY6Y783zdqRUVFc54WlqazFGdUb6c0tLSuF7LTHea+d6P6ubz5Si+bkvVUdapUyeZk56e7oxv3bpV5qguSF/nJPbm21/q2IjSCe6TyM7ZKB26UV8v3vUkunNXrSdKF66vO1F1goYgJyfHGW/evLnMWbVqlTO+ePFimfPUU0854777gLpH+Tq0+/fv74wXFBTIHDWRYeXKlTLngQcecMafeeYZmVNUVOSM33bbbTJHLfvtb38rc2bMmOGMq4kUZmabN292xn3nhuoEnjlzpsw5//zznfHTTjtN5nTv3t0Z/+yzz2TOsGHD5DIzvvEDAAAIBoUfAABAICj8AAAAAkHhBwAAEAgKPwAAgEBQ+AEAAAQiKbaf8zESOarA91pqc371q1/JHNWmvXr1apmjWuXVg5fNzFq0aOGMr1+/Pu5t843FUONcGjduLHNUi3+U0Ry+Q0KNZunYsaPM+eijj5zx999/X+aokRW+UQZR1MbxMIk818aNGyeXnXXWWc64b/SH4jueo4xZiSLK8axEGRvje5+J3Ae+cS7q/GjSpInMee6555zxSy+9NL4N24e6dK4dc8wxMufGG290xps1ayZz1P7fsmWLzFEjRnznp288jKLOm0WLFsmcZcuWOeMvv/yyzFHjuzp06CBzNm3a5IyvWbNG5qj34xttlpmZ6Yxv27ZN5mRkZDjjWVlZMke9nu++puoY37Gzr3ONb/wAAAACQeEHAAAQCAo/AACAQFD4AQAABILCDwAAIBDxP+27hvg6Wjdu3Bh3juowitJ9l+guK9WR4+tKUst83ZZq2fbt22WO6jj2adu2bdw5tbEDsC7yfZbqWPcdZ6rr3XecRzmn1Ov5um3VeZjorl61nijv03d+KlGuKb5rVF5eXtyvV9/5Jg4MGjQo7tdTXaPNmzeXOWqKRFpamszJzs52xnNzc2XO0qVLnfH33ntP5iTSkiVLqmU9Phs2bKjpTahWfOMHAAAQCAo/AACAQFD4AQAABILCDwAAIBAUfgAAAIGg8AMAAAhEnRnnsnDhQrmsTZs2ceeolvicnByZo0aZ+EYlqIcv+8ZFqPWkp6fHneN7YHSUsRRNmzZ1xn1jCXwP7laijKzA3r755hu5TB3rRUVFMkeNc2nYMLGXktTUVGfcd95EGY2SSL5tizJSRp0DFRUVcW+DOm/NzNavXx/XdiF+ZWVlccXNzJYtW1ZFW4PQ8Y0fAABAICj8AAAAAkHhBwAAEAgKPwAAgEBQ+AEAAAQiKbaf7Wa+jrWadsoppzjjvi6/VatWOeOqa9HMrHXr1s64r8tOLfN1rapt8L0f9ZBp1VVsZlZaWuqM+zrNWrZs6Yx/9913Mse3DTUtSrdlVauuc+3ee+91xo899liZo7p3fcezej++bvht27bFnaOOM9/xl8jucd+xpPaBr4Ne8V0HVDd0SUmJzDn99NPj3oYoQj7XgOq0r3ONb/wAAAACQeEHAAAQCAo/AACAQFD4AQAABILCDwAAIBAUfgAAAIHY73EuAAAAqNv4xg8AACAQFH4AAACBoPADAAAIBIUfAABAICj8AAAAAkHhBwAAEAgKPwAAgEBQ+AEAAASCwg8AACAQ/w9QhqKv8IvCDwAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Custom Dataset for your files\n",
        "A custom Dataset class must implement ```__init__, __len__```, and ```__getitem__```\n"
      ],
      "metadata": {
        "id": "QCv7FaugpCYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from torchvision.io import read_image\n",
        "\n",
        "class CustomImageDataset(Dataset):\n",
        "  def __init__(self, annotations_file, img_dir, transform = None, target_transform = None):\n",
        "    self.img_labels = pd.read_csv(annotations_file)\n",
        "    self.img_dir = img_dir\n",
        "    self.transform = transform\n",
        "    self.target_transform = target_transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.img_labels)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
        "    image = read_image(img_path)\n",
        "    label = self.img_labels.ilod[idx, 1]\n",
        "    if self.transform:\n",
        "      image = self.transform(image)\n",
        "    if self.target_transform:\n",
        "      label = self.target_transform(label)\n",
        "    return image, label"
      ],
      "metadata": {
        "id": "vi9uvJo5o-qa"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ```init``` : run once when instantiating the Dataset object\n",
        "- ```len``` : returns the number of samples in our dataset\n",
        "- ```getitem``` : loads and returns a sample from the dataset at the given index 'idx'. Based on the index, it identifies image's location on disk, converts that to a tensor using ```read_image```, retrives the corresponding label from the csv data in ```self.img_labels```, calls the transform functions on them, and returns the tensor image and corresponding label in a tuple."
      ],
      "metadata": {
        "id": "4ECBW0fHqtUm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing your data for training with DataLoaders"
      ],
      "metadata": {
        "id": "_lGc1YqOr2mB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size = 64, shuffle = True)\n",
        "test_dataloader = DataLoader(test_data, batch_size = 64, shuffle = True)"
      ],
      "metadata": {
        "id": "-mHZ5xsfqn4z"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Iterate through the DataLoader"
      ],
      "metadata": {
        "id": "75z6LBeFsTYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_features, train_labels = next(iter(train_dataloader))\n",
        "print(f\"Feature batch shapeL {train_features.size()}\")\n",
        "print(f\"Labels batch shape: {train_labels.size()}\")\n",
        "\n",
        "img = train_features[0].squeeze()\n",
        "label = train_labels[0]\n",
        "plt.imshow(img, cmap = \"gray\")\n",
        "plt.show()\n",
        "print(f\"Label: {label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "id": "qttfjchzsMj6",
        "outputId": "91258218-bd45-4848-833b-a363a4e35c34"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature batch shapeL torch.Size([64, 1, 28, 28])\n",
            "Labels batch shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAfa0lEQVR4nO3de2zV9f3H8ddpaQ8F2lNL6U0KFLww5bLJsCMqPxwNpcuMKHF4+QOMgeiKEZiXdFERt6QbS5zRMPxng7kJXhKBaBYWRSlzAwwoYThtoFQuoy2K9kKBtvR8f38QO4/c/Hw8Pe/28Hwk34Sec179fvrtl7767Tl9NxQEQSAAABIsxXoBAIBLEwUEADBBAQEATFBAAAATFBAAwAQFBAAwQQEBAExQQAAAEwOsF/BN0WhUR44cUWZmpkKhkPVyAACOgiBQW1ubioqKlJJy/uucPldAR44cUXFxsfUyAADf0aFDhzR8+PDz3t/nCigzM9N6CbiEZWRkOGfKy8udM7t373bOjBo1yjnz8ccfO2ckqaGhwSsHfN3Fvp732nNAK1as0KhRozRw4ECVlpbq/fff/1Y5fuwGS6FQyHlLS0tz3lJSUpy3AQMGOG8++7nQj0z6wvFG/3Gxz1evnGmvvPKKlixZoqVLl+qDDz7QxIkTVV5erqNHj/bG7gAA/VCvFNAzzzyj+fPn695779U111yjF154QYMGDdKf/vSn3tgdAKAfinsBdXZ2aufOnSorK/vfTlJSVFZWpq1bt571+I6ODrW2tsZsAIDkF/cC+vzzz9Xd3a38/PyY2/Pz89XY2HjW46urqxWJRHo2XgEHAJcG819EraqqUktLS8926NAh6yUBABIg7i/Dzs3NVWpqqpqammJub2pqUkFBwVmPD4fDCofD8V4GAKCPi/sVUHp6uiZNmqRNmzb13BaNRrVp0yZNmTIl3rsDAPRTvfKLqEuWLNHcuXP1wx/+UNdff72effZZtbe369577+2N3QEA+qFeKaA5c+bos88+05NPPqnGxkZ9//vf18aNG896YQIA4NIVCoIgsF7E17W2tioSiVgvo0/w+a3vPvbp7HcWL17snLnyyiudM8ePH3fODB061Dlz8OBB54wkLVu2zCsHfF1LS4uysrLOe7/5q+AAAJcmCggAYIICAgCYoIAAACYoIACACQoIAGCCAgIAmKCAAAAmKCAAgAkKCABgggICAJiggAAAJnplGjbiIxkHi/pMRB87dqxzprS01DkjnRmemAiPPPKIc+bf//63c6a2ttY5I0lPP/20c+bdd991ztTX1ztnPv30U+cM+iaugAAAJiggAIAJCggAYIICAgCYoIAAACYoIACACQoIAGCCAgIAmKCAAAAmKCAAgAkKCABgggICAJiggAAAJpiGDd12221eueHDhztnMjIynDNffPGFc6arq8s5I0mTJk1yzvhMLV+7dm1C9uN7HC6//HLnTHFxsXNm3LhxzpkBA9y/bP33v/91zkjSunXrnDO+x/xSxBUQAMAEBQQAMEEBAQBMUEAAABMUEADABAUEADBBAQEATFBAAAATFBAAwAQFBAAwQQEBAExQQAAAEwwjTTJ33HGHc6agoMBrX5988olzJhqNJiSTm5vrnJGk/Px854zPoMurrrrKObN//37nTFFRkXNG8hsA29DQ4Jw5cOCAcyYcDjtnfI63JM2ZM8c589e//tVrX5ciroAAACYoIACACQoIAGCCAgIAmKCAAAAmKCAAgAkKCABgggICAJiggAAAJiggAIAJCggAYIICAgCYYBhpHzZs2DDnzLXXXuucqampcc5IUlpamnNmwAD3Uy4lxf37pKNHjzpnJOkvf/mLcyYzM9M5c/jwYeeMz4DQnTt3OmckqbOz0zmTlZXlnBk4cKBzxud8+Oijj5wzknTdddc5Z3JycpwzPp/bZMAVEADABAUEADAR9wJ66qmnFAqFYraxY8fGezcAgH6uV54Duvbaa/X222//byceP/cHACS3XmmGAQMGeP+VTQDApaFXngPau3evioqKNHr0aN1zzz06ePDgeR/b0dGh1tbWmA0AkPziXkClpaVavXq1Nm7cqJUrV6q+vl433XST2trazvn46upqRSKRnq24uDjeSwIA9EFxL6CKigrdcccdmjBhgsrLy/W3v/1Nzc3NevXVV8/5+KqqKrW0tPRshw4diveSAAB9UK+/OiA7O1tXXXWV9u3bd877w+GwwuFwby8DANDH9PrvAR0/flx1dXUqLCzs7V0BAPqRuBfQww8/rJqaGn366af617/+pdtuu02pqam666674r0rAEA/FvcfwR0+fFh33XWXjh07pmHDhunGG2/Utm3bvOaaAQCSV9wL6OWXX473u7xk+QxCPHHihHMmEok4Z6QzL6F35TPk0segQYO8cnl5ec4Zn6GsQRA4Z3z4rE2SBg8e7Jzp7u52zvgcB59hpD5DTyW//xsMI/32mAUHADBBAQEATFBAAAATFBAAwAQFBAAwQQEBAExQQAAAExQQAMAEBQQAMEEBAQBMUEAAABMUEADARK//QTr4Gzt2rHNm//79zpns7GznjCSdOnXKOfPZZ5957cuV79DTaDTqnOnq6kpIxkd7e7tX7vTp086ZAQPcv5z4DI0NhULOmaysLOeMJB07dsw5M3LkSOfM+f5gZ7LjCggAYIICAgCYoIAAACYoIACACQoIAGCCAgIAmKCAAAAmKCAAgAkKCABgggICAJiggAAAJiggAIAJCggAYIJp2AniM8HXZ1Lwl19+6ZzxmWIsSYWFhc6ZEydOOGeam5udM6mpqc4ZSQqCwDnjM0HbJ5OoCdWJ5LO+gQMH9sJKzm337t3OmTFjxvTCSpITV0AAABMUEADABAUEADBBAQEATFBAAAATFBAAwAQFBAAwQQEBAExQQAAAExQQAMAEBQQAMEEBAQBM9O1JhUnkBz/4gXPm+PHjzpn29nbnjO/gzlGjRjlnRo8e7Zypra11zpw6dco5I/kfC1dpaWnOGZ+1+QzBlfyGsvoMz/UZLOqzn0OHDjlnJKmjo8M5M2zYMOeMz8fkM9i3r+EKCABgggICAJiggAAAJiggAIAJCggAYIICAgCYoIAAACYoIACACQoIAGCCAgIAmKCAAAAmKCAAgAmGkSbIT3/6U+dMZ2encyYvL88509zc7JyR/IZj+gyfzM7Ods74fkw+otFoQvbjM8A0PT3da18DBrh/acjIyHDOhMNh54zPcfAZKir5fUzd3d3OmWuuucY5s2PHDudMX8MVEADABAUEADDhXEBbtmzRLbfcoqKiIoVCIa1fvz7m/iAI9OSTT6qwsFAZGRkqKyvT3r1747VeAECScC6g9vZ2TZw4UStWrDjn/cuXL9dzzz2nF154Qdu3b9fgwYNVXl7u/QfCAADJyfmZxoqKClVUVJzzviAI9Oyzz+rxxx/XrbfeKkl68cUXlZ+fr/Xr1+vOO+/8bqsFACSNuD4HVF9fr8bGRpWVlfXcFolEVFpaqq1bt54z09HRodbW1pgNAJD84lpAjY2NkqT8/PyY2/Pz83vu+6bq6mpFIpGerbi4OJ5LAgD0UeavgquqqlJLS0vPdujQIeslAQASIK4FVFBQIElqamqKub2pqannvm8Kh8PKysqK2QAAyS+uBVRSUqKCggJt2rSp57bW1lZt375dU6ZMieeuAAD9nPOr4I4fP659+/b1vF1fX69du3YpJydHI0aM0KJFi/TrX/9aV155pUpKSvTEE0+oqKhIs2bNiue6AQD9nHMB7dixQzfffHPP20uWLJEkzZ07V6tXr9ajjz6q9vZ2LViwQM3Nzbrxxhu1ceNGrxlgAIDkFQqCILBexNe1trYqEolYLyPuxowZ45wZPny4cyYnJ8c5M2TIEOeM5DdAsaWlxTmzf/9+58zJkyedM5Lf0Eqf4ZM+35AdP37cOeMzVNQ35/P8rU/G53w9ePCgc0by+/80dOhQ54zPi6/Wrl3rnEm0lpaWC36OzV8FBwC4NFFAAAATFBAAwAQFBAAwQQEBAExQQAAAExQQAMAEBQQAMEEBAQBMUEAAABMUEADABAUEADBBAQEATPiNyoWzurq6hGQS6Wc/+5lzZvz48b2wkvhJS0tLyH7S09OdMz4TtFNS/L7HjEajXjlXPlO3MzIynDMHDhxwzkjSP/7xD+dMe3u7c8Z3ent/xxUQAMAEBQQAMEEBAQBMUEAAABMUEADABAUEADBBAQEATFBAAAATFBAAwAQFBAAwQQEBAExQQAAAEwwjTZBQKOScCYKgF1YSP59++qlzZvLkyc6Z1NRU54zP8fbdlw+foac+g0V9hn1KiRtG6jNY9KOPPnLOfPDBB84Z9D6ugAAAJiggAIAJCggAYIICAgCYoIAAACYoIACACQoIAGCCAgIAmKCAAAAmKCAAgAkKCABgggICAJhgGGmCJGqwaCKHnn7xxRfOmUQN1PTZjyR1d3c7Z3wGiyZq6Kkvn4/J5zzyOQ5NTU3OGV/JOES4L+EKCABgggICAJiggAAAJiggAIAJCggAYIICAgCYoIAAACYoIACACQoIAGCCAgIAmKCAAAAmKCAAgAmGkcLb559/7pzp6upyzvgMhPTJSH7DMX32lahhpKdPn/bK+QyA9Rnkmp6e7pw5cOCAc8aXz2BRBph+e1wBAQBMUEAAABPOBbRlyxbdcsstKioqUigU0vr162PunzdvnkKhUMw2c+bMeK0XAJAknAuovb1dEydO1IoVK877mJkzZ6qhoaFnW7t27XdaJAAg+Tg/01hRUaGKiooLPiYcDqugoMB7UQCA5NcrzwFt3rxZeXl5uvrqq/XAAw/o2LFj531sR0eHWltbYzYAQPKLewHNnDlTL774ojZt2qTf/va3qqmpUUVFxXlfolldXa1IJNKzFRcXx3tJAIA+KO6/B3TnnXf2/Hv8+PGaMGGCxowZo82bN2v69OlnPb6qqkpLlizpebu1tZUSAoBLQK+/DHv06NHKzc3Vvn37znl/OBxWVlZWzAYASH69XkCHDx/WsWPHVFhY2Nu7AgD0I84/gjt+/HjM1Ux9fb127dqlnJwc5eTkaNmyZZo9e7YKCgpUV1enRx99VFdccYXKy8vjunAAQP/mXEA7duzQzTff3PP2V8/fzJ07VytXrtTu3bv15z//Wc3NzSoqKtKMGTP0q1/9SuFwOH6rBgD0e84FNG3atAsOzvv73//+nRaE7yaRQw1bWlqcMz7DSNPS0hKSkaRoNOqc6ezs9NqXq8GDBztn2tvbvfblM4zUZ8Cqz/n65ZdfOmd8MVi0dzELDgBgggICAJiggAAAJiggAIAJCggAYIICAgCYoIAAACYoIACACQoIAGCCAgIAmKCAAAAmKCAAgAkKCABgIu5/khuXDp+pvz7TsNPT050zvhOJu7u7nTM+U6BPnz7tnPGZuu07FdznY/Lhcz74TCxH38QVEADABAUEADBBAQEATFBAAAATFBAAwAQFBAAwQQEBAExQQAAAExQQAMAEBQQAMEEBAQBMUEAAABMMI0VCpaS4f8/jM4w0kcLhsHNm4MCBzpmOjg7njC+foawDBrh/OQmFQs4Zn0Gu6Ju4AgIAmKCAAAAmKCAAgAkKCABgggICAJiggAAAJiggAIAJCggAYIICAgCYoIAAACYoIACACQoIAGCCYaRIqMGDBztnvvzyS+eMz2BMKXGDLocMGeKcaW9vd84EQeCckfyOn08mNTXVOZOWluac6erqcs6g93EFBAAwQQEBAExQQAAAExQQAMAEBQQAMEEBAQBMUEAAABMUEADABAUEADBBAQEATFBAAAATFBAAwATDSJFQ6enpzpmUFPfvk3wGVkpSd3e3cyYUCjlnfIZj+gwW9VmbL5/1+XxufQaYMoy0b+IKCABgggICAJhwKqDq6mpNnjxZmZmZysvL06xZs1RbWxvzmFOnTqmyslJDhw7VkCFDNHv2bDU1NcV10QCA/s+pgGpqalRZWalt27bprbfeUldXl2bMmBHzh7IWL16sN954Q6+99ppqamp05MgR3X777XFfOACgf3N6EcLGjRtj3l69erXy8vK0c+dOTZ06VS0tLfrjH/+oNWvW6Mc//rEkadWqVfre976nbdu26Uc/+lH8Vg4A6Ne+03NALS0tkqScnBxJ0s6dO9XV1aWysrKex4wdO1YjRozQ1q1bz/k+Ojo61NraGrMBAJKfdwFFo1EtWrRIN9xwg8aNGydJamxsVHp6urKzs2Mem5+fr8bGxnO+n+rqakUikZ6tuLjYd0kAgH7Eu4AqKyu1Z88evfzyy99pAVVVVWppaenZDh069J3eHwCgf/D6RdSFCxfqzTff1JYtWzR8+PCe2wsKCtTZ2anm5uaYq6CmpiYVFBSc832Fw2GFw2GfZQAA+jGnK6AgCLRw4UKtW7dO77zzjkpKSmLunzRpktLS0rRp06ae22pra3Xw4EFNmTIlPisGACQFpyugyspKrVmzRhs2bFBmZmbP8zqRSEQZGRmKRCK67777tGTJEuXk5CgrK0sPPvigpkyZwivgAAAxnApo5cqVkqRp06bF3L5q1SrNmzdPkvT73/9eKSkpmj17tjo6OlReXq4//OEPcVksACB5OBXQtxk2OHDgQK1YsUIrVqzwXhSS16BBg5wziRpgKvkNuhwwwP2p1M7OzoRkfJ9f9fmYfIaR+pwPSB7MggMAmKCAAAAmKCAAgAkKCABgggICAJiggAAAJiggAIAJCggAYIICAgCYoIAAACYoIACACQoIAGCCAgIAmPD6i6iAr8GDBztnQqGQcyYajTpnfJ0+fdo5E4lEnDM+06Y7OjqcM9KZqfauurq6ErIf30nn6Hv4TAIATFBAAAATFBAAwAQFBAAwQQEBAExQQAAAExQQAMAEBQQAMEEBAQBMUEAAABMUEADABAUEADDBMNIk4zO402fIpa9BgwY5Z8LhsHPG92NKS0tzzvgMPvX5mE6ePOmc8RkQKvkdv9TUVOfMgAHuX4J8BpieOHHCOYPexxUQAMAEBQQAMEEBAQBMUEAAABMUEADABAUEADBBAQEATFBAAAATFBAAwAQFBAAwQQEBAExQQAAAEwwjTTKJHCzqI1EDK9PT050zkpSS4v49mc8w0s7OTudMe3u7c8ZncKcknT592jnjMwjX5/PU189xfHtcAQEATFBAAAATFBAAwAQFBAAwQQEBAExQQAAAExQQAMAEBQQAMEEBAQBMUEAAABMUEADABAUEADDBMFIkVEdHh3Nm0KBBzhmfoaKS37BUn+GYGRkZzhnfwaI+fI6Dz2DRwYMHO2d8BqWib+IKCABgggICAJhwKqDq6mpNnjxZmZmZysvL06xZs1RbWxvzmGnTpikUCsVs999/f1wXDQDo/5wKqKamRpWVldq2bZveeustdXV1acaMGWf9oaz58+eroaGhZ1u+fHlcFw0A6P+cXoSwcePGmLdXr16tvLw87dy5U1OnTu25fdCgQSooKIjPCgEASek7PQfU0tIiScrJyYm5/aWXXlJubq7GjRunqqoqnThx4rzvo6OjQ62trTEbACD5eb8MOxqNatGiRbrhhhs0bty4ntvvvvtujRw5UkVFRdq9e7cee+wx1dbW6vXXXz/n+6murtayZct8lwEA6Ke8C6iyslJ79uzRe++9F3P7ggULev49fvx4FRYWavr06aqrq9OYMWPOej9VVVVasmRJz9utra0qLi72XRYAoJ/wKqCFCxfqzTff1JYtWzR8+PALPra0tFSStG/fvnMWUDgcVjgc9lkGAKAfcyqgIAj04IMPat26ddq8ebNKSkoumtm1a5ckqbCw0GuBAIDk5FRAlZWVWrNmjTZs2KDMzEw1NjZKkiKRiDIyMlRXV6c1a9boJz/5iYYOHardu3dr8eLFmjp1qiZMmNArHwAAoH9yKqCVK1dKOvPLpl+3atUqzZs3T+np6Xr77bf17LPPqr29XcXFxZo9e7Yef/zxuC0YAJAcnH8EdyHFxcWqqan5TgsCAFwamIaNhHr++eedMw899JBzpru72zkjSadOnUrIvnJzc50zmZmZzhmfj0eS1wuDIpGIc2bPnj3Omba2NueML59J5/j2GEYKADBBAQEATFBAAAATFBAAwAQFBAAwQQEBAExQQAAAExQQAMAEBQQAMEEBAQBMUEAAABMUEADARCjoY9P2WltbvYYaAl93sb/Uez4DBw50zvgMFr3sssucM6dPn3bO+Gpvb3fO7N+/3znz1d8UQ3JqaWlRVlbWee/nCggAYIICAgCYoIAAACYoIACACQoIAGCCAgIAmKCAAAAmKCAAgAkKCABgggICAJiggAAAJgZYL+Cb+thoOvRT0Wg0YTmfGW1dXV3Ome7ubueML5+PyfeYI3ld7Ot5nyugtrY26yUgCRw5ciRh+/IZwglcCtra2i44XLrPTcOORqM6cuSIMjMzFQqFYu5rbW1VcXGxDh06dMEJq8mO43AGx+EMjsMZHIcz+sJxCIJAbW1tKioqUkrK+Z/p6XNXQCkpKRcdpZ+VlXVJn2Bf4TicwXE4g+NwBsfhDOvj8G3+rA4vQgAAmKCAAAAm+lUBhcNhLV26VOFw2HoppjgOZ3AczuA4nMFxOKM/HYc+9yIEAMCloV9dAQEAkgcFBAAwQQEBAExQQAAAE/2mgFasWKFRo0Zp4MCBKi0t1fvvv2+9pIR76qmnFAqFYraxY8daL6vXbdmyRbfccouKiooUCoW0fv36mPuDINCTTz6pwsJCZWRkqKysTHv37rVZbC+62HGYN2/eWefHzJkzbRbbS6qrqzV58mRlZmYqLy9Ps2bNUm1tbcxjTp06pcrKSg0dOlRDhgzR7Nmz1dTUZLTi3vFtjsO0adPOOh/uv/9+oxWfW78ooFdeeUVLlizR0qVL9cEHH2jixIkqLy/X0aNHrZeWcNdee60aGhp6tvfee896Sb2uvb1dEydO1IoVK855//Lly/Xcc8/phRde0Pbt2zV48GCVl5fr1KlTCV5p77rYcZCkmTNnxpwfa9euTeAKe19NTY0qKyu1bds2vfXWW+rq6tKMGTPU3t7e85jFixfrjTfe0GuvvaaamhodOXJEt99+u+Gq4+/bHAdJmj9/fsz5sHz5cqMVn0fQD1x//fVBZWVlz9vd3d1BUVFRUF1dbbiqxFu6dGkwceJE62WYkhSsW7eu5+1oNBoUFBQEv/vd73pua25uDsLhcLB27VqDFSbGN49DEATB3Llzg1tvvdVkPVaOHj0aSApqamqCIDjzuU9LSwtee+21nsd8/PHHgaRg69atVsvsdd88DkEQBP/3f/8XPPTQQ3aL+hb6/BVQZ2endu7cqbKysp7bUlJSVFZWpq1btxquzMbevXtVVFSk0aNH65577tHBgwetl2Sqvr5ejY2NMedHJBJRaWnpJXl+bN68WXl5ebr66qv1wAMP6NixY9ZL6lUtLS2SpJycHEnSzp071dXVFXM+jB07ViNGjEjq8+Gbx+ErL730knJzczVu3DhVVVXpxIkTFss7rz43jPSbPv/8c3V3dys/Pz/m9vz8fH3yySdGq7JRWlqq1atX6+qrr1ZDQ4OWLVumm266SXv27FFmZqb18kw0NjZK0jnPj6/uu1TMnDlTt99+u0pKSlRXV6df/vKXqqio0NatW5Wammq9vLiLRqNatGiRbrjhBo0bN07SmfMhPT1d2dnZMY9N5vPhXMdBku6++26NHDlSRUVF2r17tx577DHV1tbq9ddfN1xtrD5fQPifioqKnn9PmDBBpaWlGjlypF599VXdd999hitDX3DnnXf2/Hv8+PGaMGGCxowZo82bN2v69OmGK+sdlZWV2rNnzyXxPOiFnO84LFiwoOff48ePV2FhoaZPn666ujqNGTMm0cs8pz7/I7jc3Fylpqae9SqWpqYmFRQUGK2qb8jOztZVV12lffv2WS/FzFfnAOfH2UaPHq3c3NykPD8WLlyoN998U++++27Mn28pKChQZ2enmpubYx6frOfD+Y7DuZSWlkpSnzof+nwBpaena9KkSdq0aVPPbdFoVJs2bdKUKVMMV2bv+PHjqqurU2FhofVSzJSUlKigoCDm/GhtbdX27dsv+fPj8OHDOnbsWFKdH0EQaOHChVq3bp3eeecdlZSUxNw/adIkpaWlxZwPtbW1OnjwYFKdDxc7Dueya9cuSepb54P1qyC+jZdffjkIh8PB6tWrg//85z/BggULguzs7KCxsdF6aQn1i1/8Iti8eXNQX18f/POf/wzKysqC3Nzc4OjRo9ZL61VtbW3Bhx9+GHz44YeBpOCZZ54JPvzww+DAgQNBEATBb37zmyA7OzvYsGFDsHv37uDWW28NSkpKgpMnTxqvPL4udBza2tqChx9+ONi6dWtQX18fvP3228F1110XXHnllcGpU6eslx43DzzwQBCJRILNmzcHDQ0NPduJEyd6HnP//fcHI0aMCN55551gx44dwZQpU4IpU6YYrjr+LnYc9u3bFzz99NPBjh07gvr6+mDDhg3B6NGjg6lTpxqvPFa/KKAgCILnn38+GDFiRJCenh5cf/31wbZt26yXlHBz5swJCgsLg/T09ODyyy8P5syZE+zbt896Wb3u3XffDSSdtc2dOzcIgjMvxX7iiSeC/Pz8IBwOB9OnTw9qa2ttF90LLnQcTpw4EcyYMSMYNmxYkJaWFowcOTKYP39+0n2Tdq6PX1KwatWqnsecPHky+PnPfx5cdtllwaBBg4LbbrstaGhosFt0L7jYcTh48GAwderUICcnJwiHw8EVV1wRPPLII0FLS4vtwr+BP8cAADDR558DAgAkJwoIAGCCAgIAmKCAAAAmKCAAgAkKCABgggICAJiggAAAJiggAIAJCggAYIICAgCYoIAAACb+H4iE8w6t9meJAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transforms"
      ],
      "metadata": {
        "id": "red_i25xs6pX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All TorchVision datasets have two parameters \n",
        "- ```transform``` to modify the features\n",
        "- ```target_transform``` to modify the labels"
      ],
      "metadata": {
        "id": "mMc8PSCttAev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda\n",
        "\n",
        "ds = datasets.FashionMNIST(\n",
        "    root = \"data\",\n",
        "    train = True,\n",
        "    download = True,\n",
        "    transform = ToTensor(),\n",
        "    target_transform= Lambda(\n",
        "        lambda y: torch.zeros(\n",
        "            10, dtype = torch.float).scatter_(0, torch.tensor(y), value = 1)\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "-1s56Rt8s22I"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ```ToTensor()``` : converts a PIL image or NumPy ```ndarray``` into a ```FloatTensor```. and scales the image's pixel intensity values in the range [0., 1.]"
      ],
      "metadata": {
        "id": "PxESEv65uhS9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build the Neural Network"
      ],
      "metadata": {
        "id": "YMmJkxjauuk1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ```torch.nn``` namespace provides all the building blocks you need to build your own nn.   \n",
        "Every module in PyTorch subclasses the ```nn.Module```"
      ],
      "metadata": {
        "id": "iQVH58Hpu68E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ],
      "metadata": {
        "id": "Xbv7HzSruQJ1"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get device for Training"
      ],
      "metadata": {
        "id": "-xw5KfcxvQoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba-hsf4St7-K",
        "outputId": "640c5ad9-c053-4f7a-d27b-c47af62ddd73"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the Class\n",
        "- define our nn by subclassing ```nn.Module```\n",
        "- initialize the nn layers in ```__init__```\n"
      ],
      "metadata": {
        "id": "JHPTMr6MvWuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.linear_relu_stack = nn.Sequential(\n",
        "        nn.Linear(28*28, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 10)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.flatten(x)\n",
        "    logits = self.linear_relu_stack(x)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "dnw1bmggvVbZ"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qi-7WIeRv7FM",
        "outputId": "48cd0e96-b7eb-4a25-f065-deedb9534443"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.rand(1, 28, 28, device = device)\n",
        "logits = model(X)\n",
        "pred_prob = nn.Softmax(dim = 1)(logits)\n",
        "y_pred = pred_prob.argmax(1)\n",
        "print(f\"Predicted class: {y_pred}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wB1PkYXkwSnY",
        "outputId": "77ecea62-3c12-4090-d85f-ff5b254758df"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: tensor([4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Layers\n",
        "- sample minibatch : 3 images of size 28*28"
      ],
      "metadata": {
        "id": "X_GKWiuKxURr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_image = torch.rand(3, 28, 28)\n",
        "print(input_image.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZ88MlRmxE8S",
        "outputId": "8ed28b8a-b7a8-42e5-b4c4-a487a50e6ce2"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```nn.Flatten``` layer : to convert each 2D 28*28 images into a contiguous array of 784 pixel values"
      ],
      "metadata": {
        "id": "9NMFvMYLxn5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flatten = nn.Flatten()\n",
        "flat_image = flatten(input_image)\n",
        "print(flat_image.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhKxDbegxjW8",
        "outputId": "16ea8974-11bc-4f16-ffc3-b3932c1d3b20"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 784])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```nn.Linear``` : a module that applies a linear transformation on the input using its stored weights and bias"
      ],
      "metadata": {
        "id": "nkudN01XyUkK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layer1 = nn.Linear(in_features = 28*28, out_features = 20)\n",
        "hidden1 = layer1(flat_image)\n",
        "print(hidden1.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kta3EYwjx8Fk",
        "outputId": "9d59040d-b544-4408-ccfa-92a1d078643d"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 20])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```nn.ReLU``` : non linear activations are what create the complex mappings between the model's inputs and outputs"
      ],
      "metadata": {
        "id": "9txVkCnIyonL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
        "hidden1 = nn.ReLU()(hidden1)\n",
        "print(f\"After ReLU: {hidden1}\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suXMMgqGykT_",
        "outputId": "7d97371a-c38c-40dc-950e-afc9aa2a5345"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before ReLU: tensor([[-1.1101e-01,  3.6770e-01,  3.5414e-01, -3.1011e-01,  1.6600e-01,\n",
            "         -2.0383e-01, -3.1476e-01,  1.9688e-01, -4.2367e-02, -8.0910e-02,\n",
            "          9.1332e-02, -3.4437e-01,  2.9712e-01, -1.3748e-02,  2.2593e-01,\n",
            "          3.4521e-01,  4.7541e-01,  5.8157e-01,  8.1083e-02,  2.7341e-01],\n",
            "        [-2.4284e-01,  4.7201e-01,  5.3104e-01, -3.3553e-01,  6.8613e-02,\n",
            "         -2.3469e-01,  1.5987e-01,  9.2000e-02, -9.4544e-02,  1.2607e-01,\n",
            "         -2.0336e-01, -5.6899e-01, -8.2157e-02, -6.0379e-02,  6.6246e-02,\n",
            "          1.7964e-01,  3.4409e-01,  2.6479e-01,  1.7856e-01,  3.9403e-01],\n",
            "        [-3.7851e-01,  4.7272e-01,  6.2125e-01, -3.7991e-01, -1.0644e-01,\n",
            "         -6.4350e-02, -1.7578e-01,  3.1275e-01, -8.2824e-02, -1.3948e-01,\n",
            "         -9.2234e-03, -3.3431e-01,  6.4032e-01,  1.9905e-01,  1.6174e-01,\n",
            "         -6.9857e-05,  3.2399e-01,  7.3774e-01,  1.4933e-01,  9.2322e-02]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "\n",
            "\n",
            "After ReLU: tensor([[0.0000, 0.3677, 0.3541, 0.0000, 0.1660, 0.0000, 0.0000, 0.1969, 0.0000,\n",
            "         0.0000, 0.0913, 0.0000, 0.2971, 0.0000, 0.2259, 0.3452, 0.4754, 0.5816,\n",
            "         0.0811, 0.2734],\n",
            "        [0.0000, 0.4720, 0.5310, 0.0000, 0.0686, 0.0000, 0.1599, 0.0920, 0.0000,\n",
            "         0.1261, 0.0000, 0.0000, 0.0000, 0.0000, 0.0662, 0.1796, 0.3441, 0.2648,\n",
            "         0.1786, 0.3940],\n",
            "        [0.0000, 0.4727, 0.6212, 0.0000, 0.0000, 0.0000, 0.0000, 0.3127, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.6403, 0.1991, 0.1617, 0.0000, 0.3240, 0.7377,\n",
            "         0.1493, 0.0923]], grad_fn=<ReluBackward0>)\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```nn.Sequential``` : an ordered container of modules"
      ],
      "metadata": {
        "id": "QtZr9lLyzKRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_modules = nn.Sequential(\n",
        "    flatten,\n",
        "    layer1,\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(20, 10)\n",
        ")\n",
        "input_image = torch.rand(3,28,28)\n",
        "logits = seq_modules(input_image)"
      ],
      "metadata": {
        "id": "dg-1gDhezHsA"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```nn.softmax``` : scaled to values [0, 1] representing the model’s predicted probabilities for each class  \n",
        "\n",
        "```dim``` parameter indicates the dimension along which the values must sum to 1."
      ],
      "metadata": {
        "id": "U6B6pcR0zWTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "softmax = nn.Softmax(dim=1)\n",
        "pred_probab = softmax(logits)"
      ],
      "metadata": {
        "id": "vLx1wwvIzSqN"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Parameters\n",
        "Subclassing ```nn.Module``` automatically tracks all fields defined inside your model object, and makes all parameters accessible using your model’s ```parameters()``` or ```named_parameters()``` methods."
      ],
      "metadata": {
        "id": "LiYPnlMFzqIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Model structure: {model}\\n\\n\")\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VSu4hYnzpIi",
        "outputId": "e10d4ed8-6bb5-4c4b-c24f-ef46b5c2ccdd"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model structure: NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "\n",
            "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[-0.0285, -0.0252, -0.0192,  ...,  0.0310, -0.0017,  0.0044],\n",
            "        [-0.0222,  0.0002, -0.0291,  ...,  0.0041,  0.0011,  0.0105]],\n",
            "       grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([-0.0348,  0.0225], grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[-0.0141, -0.0279,  0.0263,  ...,  0.0162,  0.0358, -0.0293],\n",
            "        [ 0.0411, -0.0014,  0.0123,  ..., -0.0078, -0.0322,  0.0031]],\n",
            "       grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([-0.0119,  0.0379], grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[-0.0007,  0.0073,  0.0008,  ...,  0.0325, -0.0027, -0.0102],\n",
            "        [ 0.0315, -0.0235, -0.0167,  ...,  0.0252, -0.0381, -0.0042]],\n",
            "       grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0156,  0.0336], grad_fn=<SliceBackward0>) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Automatic differentiation with ```torch.autograd```"
      ],
      "metadata": {
        "id": "2sAusUqX0Afg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "x = torch.ones(5) # input tensor\n",
        "y = torch.zeros(3) # expected output\n",
        "w = torch.randn(5, 3, requires_grad = True)\n",
        "b = torch.randn(3, requires_grad = True)\n",
        "\n",
        "z = torch.matmul(x, w) + b\n",
        "\n",
        "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
      ],
      "metadata": {
        "id": "glzyKYxsz76B"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Gradient function for z = {z.grad_fn}\")\n",
        "print(f\"Gradient function for loss = {loss.grad_fn}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPFVwfVu0uL0",
        "outputId": "8f2a3bad-1db8-42b9-f6d5-1b9cb0834bfc"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient function for z = <AddBackward0 object at 0x7ff86d2d83d0>\n",
            "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x7ff86d2d8cd0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computing gradients"
      ],
      "metadata": {
        "id": "uBpSFgvp0zOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward()\n",
        "print(w.grad)\n",
        "print(b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ernVXNVj0xEd",
        "outputId": "59cc4b0a-d432-4fd4-970f-2096a802374c"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2780, 0.2659, 0.2133],\n",
            "        [0.2780, 0.2659, 0.2133],\n",
            "        [0.2780, 0.2659, 0.2133],\n",
            "        [0.2780, 0.2659, 0.2133],\n",
            "        [0.2780, 0.2659, 0.2133]])\n",
            "tensor([0.2780, 0.2659, 0.2133])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Disabling Gradient TRacking\n",
        "all tensors with ```requires_grad=True``` are tracking their computational history and support gradient computation.  \n",
        "However, there are some cases when we do not need to do that, for example, when we have trained the model and just want to apply it to some input data, i.e. we only want to do forward computations through the network. We can stop tracking computations by surrounding our computation code with ```torch.no_grad()``` block:\n",
        "\n"
      ],
      "metadata": {
        "id": "DKbAeaqc08Bi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "z = torch.matmul(x, w)+b\n",
        "print(z.requires_grad)\n",
        "\n",
        "with torch.no_grad():\n",
        "    z = torch.matmul(x, w)+b\n",
        "print(z.requires_grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_1w0GtX06gB",
        "outputId": "5fcbf4bc-f046-468e-b44b-4fe7f45555f3"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z = torch.matmul(x, w)+b\n",
        "z_det = z.detach()\n",
        "print(z_det.requires_grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEPVgVZ51SDp",
        "outputId": "6edc6964-06d2-47f8-ad07-15806687981f"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimization"
      ],
      "metadata": {
        "id": "-zGPiV_N2VOs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prequisite Code"
      ],
      "metadata": {
        "id": "Ehu1hmvD2fWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=64)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64)\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork()"
      ],
      "metadata": {
        "id": "kTlD4Y5G2XrL"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters\n",
        "- **# of Epochs** : the number times to iterate over dataset\n",
        "- **Batch Size** : the number of data smaples propagated through the network before the parameters are updated\n",
        "- **Learning Rate** : how much to update models parameters at each batch/epoch. Smaller values yield slow learning speed, while large values may result in unpredictable behavior during training."
      ],
      "metadata": {
        "id": "8FsC566i2nIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-3\n",
        "batch_size = 64\n",
        "epochs = 5"
      ],
      "metadata": {
        "id": "CsGgc7m92jze"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**epoch** : each iteration of the optimization loop  \n",
        "- The train loop :  iterate over the training dataset and try to converge to optimal parameters\n",
        "- The validation/Test loop : iterate over the test dataset to check if model performance is improving"
      ],
      "metadata": {
        "id": "vZjtfw5F3Obc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss Function\n",
        "**Loss function** measures the degree of dissimilarity of obtained result to the target value, and it is the loss function that we want to minimize during training."
      ],
      "metadata": {
        "id": "RKbMBP4c3bUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the loss function\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "y1dZOYXS3FB6"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizer\n",
        "**Optimization** is the process of adjusting model parameters to reduce model error in each training step."
      ],
      "metadata": {
        "id": "dltM-3Sj3mTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "yUJyYCz43roV"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ```optimizer.zero_grad()``` : to reset the gradients of model parameters\n",
        "- ```loss.backward()``` : backpropagates\n",
        "- ```optimizer.step()``` : adjust the parameters by gradients collected in the backward pass"
      ],
      "metadata": {
        "id": "Hw6Dpv_23v1D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Full Implementation"
      ],
      "metadata": {
        "id": "kQMyBWmD4NXz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "yJ_Vl1JU3uRK"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8mkhbgZ4RDg",
        "outputId": "3b573702-53e7-4207-ccf3-b4f351afeaae"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.305724  [   64/60000]\n",
            "loss: 2.284523  [ 6464/60000]\n",
            "loss: 2.265043  [12864/60000]\n",
            "loss: 2.252532  [19264/60000]\n",
            "loss: 2.236014  [25664/60000]\n",
            "loss: 2.214025  [32064/60000]\n",
            "loss: 2.219948  [38464/60000]\n",
            "loss: 2.185939  [44864/60000]\n",
            "loss: 2.177628  [51264/60000]\n",
            "loss: 2.132831  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 42.4%, Avg loss: 2.131132 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.150283  [   64/60000]\n",
            "loss: 2.131153  [ 6464/60000]\n",
            "loss: 2.068864  [12864/60000]\n",
            "loss: 2.083009  [19264/60000]\n",
            "loss: 2.034509  [25664/60000]\n",
            "loss: 1.968132  [32064/60000]\n",
            "loss: 2.005025  [38464/60000]\n",
            "loss: 1.915214  [44864/60000]\n",
            "loss: 1.921957  [51264/60000]\n",
            "loss: 1.840767  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 59.1%, Avg loss: 1.839124 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.879416  [   64/60000]\n",
            "loss: 1.840834  [ 6464/60000]\n",
            "loss: 1.720080  [12864/60000]\n",
            "loss: 1.763539  [19264/60000]\n",
            "loss: 1.646728  [25664/60000]\n",
            "loss: 1.605360  [32064/60000]\n",
            "loss: 1.631273  [38464/60000]\n",
            "loss: 1.527121  [44864/60000]\n",
            "loss: 1.557574  [51264/60000]\n",
            "loss: 1.445460  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 61.1%, Avg loss: 1.464633 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.539072  [   64/60000]\n",
            "loss: 1.499552  [ 6464/60000]\n",
            "loss: 1.350881  [12864/60000]\n",
            "loss: 1.424936  [19264/60000]\n",
            "loss: 1.299466  [25664/60000]\n",
            "loss: 1.303791  [32064/60000]\n",
            "loss: 1.321347  [38464/60000]\n",
            "loss: 1.242572  [44864/60000]\n",
            "loss: 1.284167  [51264/60000]\n",
            "loss: 1.181310  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 63.3%, Avg loss: 1.208347 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.287630  [   64/60000]\n",
            "loss: 1.269566  [ 6464/60000]\n",
            "loss: 1.105675  [12864/60000]\n",
            "loss: 1.214360  [19264/60000]\n",
            "loss: 1.086034  [25664/60000]\n",
            "loss: 1.112169  [32064/60000]\n",
            "loss: 1.138786  [38464/60000]\n",
            "loss: 1.070862  [44864/60000]\n",
            "loss: 1.119695  [51264/60000]\n",
            "loss: 1.033121  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 65.1%, Avg loss: 1.054295 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.122980  [   64/60000]\n",
            "loss: 1.128803  [ 6464/60000]\n",
            "loss: 0.947114  [12864/60000]\n",
            "loss: 1.085302  [19264/60000]\n",
            "loss: 0.958772  [25664/60000]\n",
            "loss: 0.985292  [32064/60000]\n",
            "loss: 1.027991  [38464/60000]\n",
            "loss: 0.963594  [44864/60000]\n",
            "loss: 1.014958  [51264/60000]\n",
            "loss: 0.942515  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 66.3%, Avg loss: 0.956426 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.009399  [   64/60000]\n",
            "loss: 1.038339  [ 6464/60000]\n",
            "loss: 0.839496  [12864/60000]\n",
            "loss: 0.999707  [19264/60000]\n",
            "loss: 0.878731  [25664/60000]\n",
            "loss: 0.896566  [32064/60000]\n",
            "loss: 0.955434  [38464/60000]\n",
            "loss: 0.894186  [44864/60000]\n",
            "loss: 0.943406  [51264/60000]\n",
            "loss: 0.881715  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 67.6%, Avg loss: 0.889794 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.925815  [   64/60000]\n",
            "loss: 0.975012  [ 6464/60000]\n",
            "loss: 0.762479  [12864/60000]\n",
            "loss: 0.938860  [19264/60000]\n",
            "loss: 0.824952  [25664/60000]\n",
            "loss: 0.831931  [32064/60000]\n",
            "loss: 0.903737  [38464/60000]\n",
            "loss: 0.847233  [44864/60000]\n",
            "loss: 0.891732  [51264/60000]\n",
            "loss: 0.837722  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 69.1%, Avg loss: 0.841614 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.861567  [   64/60000]\n",
            "loss: 0.927047  [ 6464/60000]\n",
            "loss: 0.704938  [12864/60000]\n",
            "loss: 0.893319  [19264/60000]\n",
            "loss: 0.786159  [25664/60000]\n",
            "loss: 0.783533  [32064/60000]\n",
            "loss: 0.864021  [38464/60000]\n",
            "loss: 0.813972  [44864/60000]\n",
            "loss: 0.852497  [51264/60000]\n",
            "loss: 0.803836  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 70.3%, Avg loss: 0.804748 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.810141  [   64/60000]\n",
            "loss: 0.888207  [ 6464/60000]\n",
            "loss: 0.660124  [12864/60000]\n",
            "loss: 0.857688  [19264/60000]\n",
            "loss: 0.756482  [25664/60000]\n",
            "loss: 0.746233  [32064/60000]\n",
            "loss: 0.831478  [38464/60000]\n",
            "loss: 0.789001  [44864/60000]\n",
            "loss: 0.821390  [51264/60000]\n",
            "loss: 0.776339  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 71.7%, Avg loss: 0.775089 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save & Load the Model"
      ],
      "metadata": {
        "id": "vNn2kUN44bbz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.models as models"
      ],
      "metadata": {
        "id": "0h8wrpZ24Sk0"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving and Loading Model Weights"
      ],
      "metadata": {
        "id": "OvAztufj4gnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.vgg16(weights='IMAGENET1K_V1')\n",
        "torch.save(model.state_dict(), 'model_weights.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "785L6A5S4f1T",
        "outputId": "f1a29f78-1b3e-4c0b-f370-529240107502"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:06<00:00, 79.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.vgg16() # we do not specify weights, i.e. create untrained model\n",
        "model.load_state_dict(torch.load('model_weights.pth'))\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sU1Jf5f4mbu",
        "outputId": "3590dd44-3653-4659-e61f-73d27ba4c1d2"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): ReLU(inplace=True)\n",
              "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (25): ReLU(inplace=True)\n",
              "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (27): ReLU(inplace=True)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "be sure to call ```model.eval()``` method before inferencing to set the dropout and batch normalization layers to evaluation mode. Failing to do this will yield inconsistent inference results."
      ],
      "metadata": {
        "id": "1kPmdS8f4zbJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving and Loading Models with Shapes"
      ],
      "metadata": {
        "id": "CQ_ai3ve43il"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, 'model.pth')"
      ],
      "metadata": {
        "id": "2NzVgTUz47P5"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load('model.pth')"
      ],
      "metadata": {
        "id": "2mDyOpCT5EXv"
      },
      "execution_count": 77,
      "outputs": []
    }
  ]
}